[
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Blog",
    "section": "",
    "text": "Date\n\n\nTitle\n\n\nAuthor\n\n\n\n\n\n\nAug 4, 2024\n\n\n3D tessellation of biomolecular cavities\n\n\nArtem Kushner, Khanh Dao Duc\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "артём",
    "section": "",
    "text": "CV\nPapers\nBlog\nLists\n\n\n  \n\n\n\n\nHowdy, this is my personal website. I’m a research technician at IMP, Vienna with Sven Klumpe and at UBC, Vancouver with Khanh Dao Duc.\nMy general interests are in biology, software and mathematics. I have done work in structural biology infrastructure (atomic models,CryoET), ML applications to cryoEM, some heterogeneity problems, shape analysis. I built and maintain ribosome.xyz.\nOutside of academia, I’ve had a few backend roles. I enjoy running, hiking, canoeing, music, history, economics. I planted around 250,000 trees in the north-west of Canada between 2016 and 2018. My favorite books are by Proust, Eco and McCarthy. I live in Vienna now.\nI can be reached at rtkushner@gmail.com."
  },
  {
    "objectID": "listings/architecture_and_bio.html",
    "href": "listings/architecture_and_bio.html",
    "title": "артём",
    "section": "",
    "text": "Hey gemini. I’m interested in writing an essay with my friend that would explore and reveal the connections between cell/structural/evolutionary/etc biology and architecture/carpentry.\nI’m a structural biologist and my expertise is mostly in dealing with 3d strucutre of biomolecules, microscopes, machine learning models and the philosophy of science that underlies those fields.\nMy friend is an architecture student with a passion for smart sustainable and robust housing: he is adept at both the materials and building specification of modern buildings, but his deep research interests also extend into the socio-economical aspects of home-building. For example he has a well-developed narrative and theory about how to both design and build whole quarterblocks from the ground up sustainably while providing sovereignty to people who are going to inhabit the resulting building (as opposed to ruthlessly mercantile system that rules in north america in the last 20-30 years with real estate projects being viewed more as a financial asset and the housing crisis and the unimaginative copypasted condominius everywhere etc). He has a very fine grained idea (a lot better articulated than i can here) of how to subdivide both the area and space of the future dwelling into appropriate units as well as how the “pipeline” or the process of acquiring materials, refining budgets, all stages of developed from pouring the concrete foundation to putting finish on the door frames on the last floor, how dynamic this process can be etc. He also has a very good grasp of the problems that plague the architecture and carpentry education and “philosophy” these days.\nFor my part im just a structural biology programmer.\nWe had a nice chat a few times where we often stumbled into the basic idea of a building being somewhat like a biological cell: not just in a superficial way of housing other things but in the way that both evolve in a platonic sense (evolution of life and of human’s engineering) and in the singular sense ( the building has many inhabitants that change and bring their cultures, habits, etc), both are embedded in the network of their neighbors, both have a myriad of architectural constituents and parts whose materials are diverse and whose compositions determine the identity of the whole. We are also very interested in methodologies that people use to study and develop their understanding/capacity to engieer both: for example nomenclature of certain in architecture is both very precise, but also varies regionally and between countries. Same in biology for example the nomenclature of new families of proteins and genes is a very pernicious problem that can be both a unifying and consolidating aspect in the collaborative nature of science or a source of discord and misunderstanding. Im not interested in the theoretical and descriptive aspects of these two fields and their subjects however, but also in men’s applied practice of them: the connections/parallels/differences in economical flows ( contractors, acquisition of materials, transportation, construction etc. in architecture) and like (microscopy, genetic engineering, philosophical aspects of the perpetually mapped identity of the cell, the databases that track the multiplicities of biological data). I’m also interested in the anecdotes or facts that point out some quirks in both field that unite both and perhaps also speak to and elucidate the general tendency of human mind to map and subdivide the “problem” of spatial theaters: the ones which inhabit and the other that constitutes us.\nThis last bit is more or less i guess what i’m going for with this blog/essay.\nI need your help pulling and collating info and suggesting directions/topics to explore further, tangible examples to muster and lenses through which to see the following: the act of designing, architecting and painstakingly constructing something as multifaceted as a building, a house, a library, a part of a city and the ease with which humans have come to be abled to do it involve a lot more than just arranging materials in a certain shape. What are the “blind spots” in the methodologies of the fields which embed and inform the act of building itself? (ex. material sciences, sociology, history, politics, economics, logistics, ontology of a given region). In a movement of knowledge inverse to that of architecture, the aim of biology and its subfields is in deconstructing the subject that exists in the world already. A body of knowledge grows by perturbing a cell, reaching deeper into the sub-angstrom scale, mapping more and more rooms and tenats in the gargantuan complex that each cell is (with its ribosomes, tubulins, myriad proteins, their functions and motions etc., like a city). But this would not be possible without the wetlab methods, a whole industry of producing micrscopes reliant on theory of optics and EM radiation, computers and databases and mathematical method to bind, discretize, give meaning and cast the actors and the interiors into words, steal frames from the film of their invisible ceaseless motion deep down and in. How do these fields inform and steer and inadvertaly bind biology in a similar or completely differnet way to architecture and what can we learn, again, about how men have chosen to allocate their resources and down which avenues to send the train of their thought.\nA couple of anecdotes\n\nBridging Worlds: A Bio-Architectural Exploration of Structure, Evolution, and Human Endeavor I. Introduction: The Living Building and the Built Cell\nThe pursuit of understanding complex systems, whether naturally occurring or human-engineered, often leads to the drawing of parallels across seemingly disparate domains. The conceptualization of a building as analogous to a biological cell, extending beyond mere superficial resemblances, offers a profound lens through which to examine shared principles of organization, adaptation, and human endeavor. This report explores these deep connections between biological systems (cellular, structural, and evolutionary biology) and architectural/carpentry practices, delving into theoretical frameworks, applied methodologies, economic underpinnings, and inherent limitations. Setting the Stage: The Inherent Analogy Between Biological and Architectural Systems\nThe very foundation of cell biology reveals an early and enduring influence from architectural thought. The term “cell” itself was coined by Robert Hooke in 1665, inspired by the small, honeycomb-like compartments observed in cork, reminiscent of the monastic “cells” or small rooms. This initial structural comparison laid the groundwork for a continuous evolution of the cell metaphor, progressing from a simple “empty chamber” or “building stone” to more complex, functional analogies such as a “chemical laboratory” or a “factory”. In contemporary systems biology, the cell is increasingly understood as a “dynamic network” of interacting components. This progression from static structural comparison to a dynamic, functional, and systemic understanding in biology mirrors a similar trajectory in architectural thought, moving beyond static forms to consider buildings as evolving, responsive entities.\nThis reciprocal influence highlights a fundamental cognitive tendency of the human mind: when confronted with complex, unfamiliar systems, or tasked with designing novel ones, there is a natural inclination to map them onto familiar, well-understood structures and processes. This means that not only does biology inspire architectural design, but human-engineered and social systems also profoundly shape how biological phenomena are conceptualized and described. This constant cross-domain mapping makes the exploration inherently self-referential and philosophically rich, suggesting that human understanding of the natural world is often filtered and structured by the built world, and vice-versa. Beyond Metaphor: Unpacking Deep Structural, Evolutionary, and Functional Parallels\nThe connections between biology and architecture extend far beyond superficial likenesses, touching upon fundamental principles of organization and behavior. Structural biology, for instance, is dedicated to elucidating the “architecture of living matter” at the molecular scale, including the intricate three-dimensional structures of proteins, nucleic acids, lipid membranes, and carbohydrates. This field aims to understand how a protein’s one-dimensional amino acid sequence dictates its precise three-dimensional structure, which is absolutely essential for its biological function. This process directly parallels how architectural blueprints, often two-dimensional representations, are translated into complex three-dimensional buildings. The insights gained from structural biology reveal fundamental “design principles of living systems”.\nA core concept common to both domains is hierarchical organization, where complexity arises from the assembly of simpler units into progressively more intricate systems. This principle is evident from molecular motifs (e.g., the helix-turn-helix motif in proteins) to organ systems in biology, and from raw materials to entire city blocks in architecture. The existence of these organizational principles at vastly different scales suggests underlying, potentially universal, rules of efficiency and robustness. For example, tensegrity, a structural principle found in both biological structures and large-scale architectural designs, exemplifies how a balance of tension and compression can create strong and resilient forms with minimal material. This demonstrates a principle that transcends vast differences in scale and material composition, pointing towards a limited set of fundamental “design principles” that govern complex systems, regardless of their origin.\nThe Human Element: How Our Minds Shape and Are Shaped by These Spatial Theaters\nHuman cognition plays a pivotal role in the interplay between biological and architectural domains. The historical tendency of architects and designers to draw inspiration from biology, not merely for aesthetic forms but for methods analogous to natural growth and evolution, underscores a deep-seated human drive to find patterns and solutions in the natural world. This is notably evident in the rise of biomimicry, where nature’s strategies are emulated for sustainable design , and in the application of genetic algorithms in architectural design to generate and optimize forms.\nFurthermore, the concept of “co-evolutionary architecture” directly addresses the mutual influence and adaptation between human culture and the built environment. This mirrors the gene-culture co-evolution observed in biology, where genetic and cultural factors interact over long periods to shape human characteristics. This perspective views built spaces not as static containers but as dynamic entities that both reflect and shape human societies and behaviors. The consistent employment of biological analogies in architectural design and, conversely, architectural and industrial metaphors in biological conceptualization, points to a fundamental cognitive strategy: to understand or create complex systems, the human mind seeks to map them onto familiar, well-understood structures or processes. This cross-domain mapping is a powerful engine for innovation and knowledge transfer, but it also carries the inherent risk of introducing limitations when analogies are overextended or fail to account for unique domain-specific complexities. The inherent human drive to seek and apply patterns across domains is thus a double-edged sword, simultaneously facilitating groundbreaking innovation and creating inherent limitations in understanding, which necessitates critical self-reflection within both disciplines.\n\nStructural and Organizational Parallels: From Molecules to Megastructures\n\nThe fundamental organization of both living systems and human-made structures exhibits striking similarities, particularly in their hierarchical assembly and load-bearing strategies. Understanding these parallels provides a framework for cross-disciplinary learning and highlights shared challenges in managing complexity. Hierarchical Organization: Comparing Biological Hierarchies (Atoms to Biosphere) with Architectural Scales (Materials to City Blocks)\nBoth biological and architectural systems are fundamentally structured as complex, multi-scale hierarchies. Each level in these hierarchies is composed of units from the level below, and critically, new properties emerge at higher levels that are not present in their constituent parts. This concept of emergence is central to understanding how complexity arises from simplicity in both natural and artificial systems.\nBiological organization spans from the most basic units like atoms and molecules, progressing to macromolecules, organelles, cells, tissues, organs, organ systems, and ultimately, the entire organism. Beyond the individual, biological hierarchies extend to populations, communities, ecosystems, and the biosphere. At each step, emergent properties manifest; for instance, a cell possesses the ability to carry out all life activities, a function its individual organelles cannot achieve independently. Similarly, an organ system performs integrated functions vital for survival, which individual organs cannot accomplish alone.\nIn parallel, architecture can be conceptualized in a similar hierarchical manner. It begins with raw materials, which are processed into manufactured components and assembled into elements like walls and beams. These elements form rooms and spaces, which then combine to create buildings. Buildings, in turn, form city blocks, districts, and ultimately contribute to the global built environment. A building, as a system, is composed of subsystems (e.g., structural frame, HVAC, plumbing) and is nested within larger suprasystems (e.g., a neighborhood, a city). Just as in biology, each architectural level exhibits emergent properties: material properties contribute to structural integrity, and the arrangement of spaces creates a unique spatial experience. The aggregation of buildings forms an urban fabric, influencing social interaction and contributing to urban sustainability.\nThe hierarchical nature of both domains presents a significant challenge in inter-level communication and optimization. Understanding, designing, and optimizing these complex systems necessitates effective communication and coordination across distinct hierarchical levels. In biology, intricate cellular communication and signaling pathways are vital for coordinating activity from molecular processes to organismal functions. The contemporary view of the cell as a “dynamic network” emphasizes these complex, interacting levels and the importance of their coordinated function. In architecture, the multi-scale coordination challenges are evident in the process of subdividing dwelling spaces, acquiring materials, managing budgets, and overseeing various construction stages. A significant limitation in both fields can be the difficulty in translating insights or optimizing at one level to higher, emergent levels. For example, optimizing a material’s properties in material science does not automatically guarantee optimal building performance, which is an emergent property of the entire structure. This underscores the critical need for “systems thinking” that transcends purely reductionist approaches, acknowledging the complex interplay between hierarchical levels.\nTo illustrate these parallels, the following table provides a comparative overview of hierarchical organization in biology and architecture:\nTable 1: Hierarchical Organization: Biology vs. Architecture Level of Organization Biological Example Architectural Example Emergent Property (Biology) Emergent Property (Architecture) Fundamental Unit Atoms Raw Materials Chemical properties Material properties Sub-system Molecules Components/Elements Molecular interactions Structural integrity System Organelles Rooms/Spaces Cellular functions Spatial experience Supra-system Cells Buildings Tissue specialization Building performance Macro-system Tissues Blocks/Neighborhoods Organ function Urban fabric Global System Organism Districts Homeostasis Social interaction Ecological Level Population Cities Evolution Urban sustainability Broader Ecological Community Urban Regions Population dynamics Regional identity Planetary Scale Ecosystem Global Built Environment Interspecies relationships Global impact Tensegrity and Load Bearing: Principles of Tension and Compression in Biological Structures and Architectural Design\nBeyond simple stacking or continuous compression, complex systems often employ sophisticated load-bearing strategies that distribute forces efficiently. Tensegrity, a portmanteau of “tensional integrity,” is a prime example of such a principle found across vast scales and diverse domains. This structural concept is based on a system of isolated components under compression (typically rigid bars or struts) held within a continuous network of tension (usually flexible cables or tendons), where the compressed members do not directly touch each other. This unique arrangement ensures that no structural member experiences bending moments or shear stresses, leading to exceptional strength, resilience, and flexibility with minimal material usage.\nIn biological systems, the human body’s musculoskeletal system serves as a compelling example of “biotensegrity”. Bones provide discontinuous compression, while a continuous network of connective tissues—muscles, fascia, ligaments, and tendons—maintains tension throughout the system. At the cellular level, the cytoskeleton, which dictates cell shape and its response to external mechanical forces, can also be mathematically modeled as a tensegrity structure. This biological application highlights how living systems achieve their remarkable adaptability and robustness.\nIn architecture, tensegrity principles have been applied to design and construct iconic large-scale structures, demonstrating its practical utility in achieving efficient and resilient designs. Notable examples include the roofs of the Spodek arena complex, the Seoul Olympic Gymnastics Arena, the Georgia Dome, and Tropicana Field, as well as the Kurilpa Bridge, which is currently the world’s largest tensegrity bridge. These structures showcase how the principle of discontinuous compression and continuous tension can create lightweight yet incredibly strong forms.\nThe application of tensegrity in both biology and architecture reveals a profound structural principle and highlights a potential area of oversight in traditional design thinking. From a conventional architectural perspective, which often prioritizes rigid, continuous compression and mass, a structure where primary load-bearing elements do not directly connect might intuitively appear unstable or fragile. However, the strength of tensegrity lies in its intelligent distribution of forces through a continuous tension network, allowing compressed elements to “float” and distribute stress efficiently. This challenges conventional notions of structural stability and material usage. Embracing tensegrity more broadly in architecture could lead to the development of more lightweight, adaptable, and resource-efficient buildings, aligning with sustainable design goals and potentially reducing overall material consumption. This principle encourages designers to look beyond conventional material and structural paradigms to achieve optimal performance.\nMateriality and Composition: Diverse Materials and Their Assembly in Biomolecules versus Modern Buildings\nThe fundamental building blocks of both living organisms and human-made structures come together through intricate processes to form complex, functional wholes. The manner in which these materials are acquired, processed, and assembled reveals both striking parallels and significant divergences between biological and architectural practices.\nIn biology, biomolecules like proteins and nucleic acids exhibit a remarkable hierarchical assembly, progressing from primary sequences to secondary, tertiary, and quaternary structures, ultimately forming large, intricate complexes. This assembly is frequently driven by “molecular self-assembly”—a spontaneous and reversible organization of components through specific local interactions, occurring without external direction. This inherent ability of biological systems to build themselves from the bottom up, driven by intrinsic properties and environmental cues, stands in sharp contrast to traditional human construction, which is largely a top-down, externally directed process involving the deliberate placement and joining of components.\nHowever, contemporary architecture is increasingly exploring the use of “biomaterials”—materials derived from living organisms or nature—such as mycelium bricks, algae facades, and silk pavilions. These materials often leverage biological processes for their unique properties. For instance, mycelium, the root structure of fungi, can be grown into bricks using agricultural waste as a substrate, forming a self-supporting composite. Similarly, algae can be cultivated in facade systems to generate heat and biomass for renewable energy. Furthermore, the development of “self-healing materials” in construction, such as concrete with embedded bacteria that produce calcium carbonate to seal cracks, is directly inspired by biological self-repair mechanisms. These innovations represent a significant shift in architectural materiality, moving away from inert, static materials towards dynamic, living, or bio-inspired ones.\nThe emergence of these new materials signifies a fundamental shift in construction from “making” to “growing” or “cultivating.” Many of these new materials leverage biological “growth” or “self-repair” processes rather than purely mechanical assembly. This paradigm shift has profound implications for the entire construction lifecycle, including logistics, waste management, and long-term sustainability. If buildings can “grow” or “repair themselves,” it challenges current linear supply chain models that rely on constant extraction and disposal. Such an approach aligns seamlessly with circular economy principles, where materials are kept in use for as long as possible, waste is eliminated, and natural systems are regenerated. This also exposes a limitation in traditional construction education and practice, which primarily focuses on inert, manufactured materials rather than dynamic, living ones that can adapt and regenerate over time.\n\nEvolutionary Dynamics: Adaptation, Change, and Design\n\nThe concept of evolution, typically associated with biological life, finds compelling parallels in the development and adaptation of human-made structures. This section explores how both biological and architectural systems undergo processes of change, whether in a broad, “Platonic” sense of design principles evolving, or in a more singular sense of individual entities adapting to dynamic environments and inhabitants. Evolution in a Platonic Sense: The Evolution of Life vs. the Evolution of Human Engineering and Design Principles\nBoth biological life and human-made artifacts exhibit patterns of change and adaptation over time, suggesting a broader “evolution” of design principles. In biology, natural selection, as articulated by Charles Darwin, explains the complex organization and functionality of organisms as the result of natural processes—the gradual accumulation of spontaneously arisen variations sorted by environmental pressures. This concept of “design without a designer” brought the origin and adaptations of organisms into the realm of scientific inquiry.\nIn architecture, a similar, albeit often unconscious, evolutionary process has shaped building forms throughout history. From ancient structures like the Walls of Jericho to the soaring Gothic cathedrals, the evolution of architectural forms can be seen as a continuous development of structural elements (e.g., post-and-lintel systems, arches, vaulting) driven by available materials, tools, and structural considerations. This historical progression, where successful structural innovations and building techniques are retained and refined, parallels biological evolution, which is shaped by environmental pressures, resource availability, and genetic variation.\nMore recently, this analogy has been consciously applied through “biomimicry,” a formalized approach to design problems inspired by natural elements and processes, explicitly aiming to enhance sustainability and create regenerative structures. Architects and designers have sought inspiration from biology since the early 19th century, not just by imitating forms but by adopting methods analogous to natural growth and evolution. This deliberate learning from nature represents a conscious acceleration of design evolution. Modern architectural design is now leveraging this analogy through “genetic algorithms” and other computational methods to emulate natural evolution for form generation and optimization. These algorithms can generate numerous adaptive solutions and allow for dynamic evaluation criteria, effectively speeding up the design process.\nThe contrast between the historical, often unconscious, evolution of architectural forms and the modern, conscious application of biomimicry reveals a critical point. While historical architectural styles emerged from practical constraints and incremental innovations, biomimicry represents a deliberate effort to abstract and apply principles from nature’s time-tested evolutionary solutions. This highlights a limitation in traditional architectural education that might over-emphasize historical styles and aesthetic movements without sufficiently exploring the underlying evolutionary pressures and adaptive strategies that shaped them. Conscious biomimicry attempts to accelerate and optimize this evolutionary process in design, but its effectiveness is often constrained by challenges in cross-domain information transfer and fragmented terminology between biologists and architects.\nEvolution in a Singular Sense: How Individual Buildings and Cells Adapt to Changing Inhabitants, Cultures, and Environments\nBeyond the broad, species-level evolution of design principles, individual entities—be they biological cells or human-made buildings—undergo continuous adaptation and change in response to their dynamic internal and external environments. A biological cell is in a constant state of flux, adapting to its microenvironment, nutrient availability, and internal signaling states. This dynamic responsiveness is crucial for its survival and function.\nSimilarly, buildings, once constructed, are not static monuments but dynamic entities that interact continuously with their inhabitants and the surrounding environment. The concept of “adaptive architecture” directly addresses this, focusing on designing flexible, responsive, and sustainable spaces that can adjust to changing environmental conditions and user needs over time. This approach recognizes that the needs of inhabitants and the external climate are not fixed but evolve.\n“Co-evolutionary architecture” extends this idea further, positing that architecture evolves through a mutual influence and adaptation between culture and nature, life and technology, constantly changing itself. This concept parallels gene-culture co-evolution in human biology, where genes and culture interact over long periods to shape human characteristics. The observation that a building “has many inhabitants that change and bring their cultures, habits, etc.” directly speaks to this dynamic, singular evolution of a built structure. The static design of a building, often envisioned as a final product, stands in tension with the dynamic reality of its use and occupancy.\nTraditional architectural practice often culminates in a fixed, static design and construction, aiming for a completed, unchanging form. However, the reality is that buildings are inhabited and used by dynamic human and natural systems. This highlights a fundamental limitation in architectural practice: the underestimation or neglect of continuous, dynamic interaction and adaptation post-construction. Applying biological principles more deeply could lead to buildings designed with inherent flexibility, modularity , and responsiveness, moving away from a system that views real estate as a static financial asset. This also strongly links to the principles of a circular economy, where adaptability, reuse, and regeneration are paramount throughout a product’s lifecycle. Designing for change, rather than for stasis, becomes a critical objective.\nNatural Selection in Design: Exploring the “Design Without a Designer” Concept in Biology and Its Implications for Architectural Innovation\nThe seemingly purposeful “design” of both organisms and artifacts can be understood through processes akin to natural selection, even without a conscious “designer” in the traditional sense. In biology, Charles Darwin’s theory of natural selection explains the complex organization and functionality (“design”) of organisms as the result of natural processes—the gradual accumulation of spontaneously arisen variations (mutations) sorted by natural selection. This concept of “design without a designer” brought the origin and adaptations of organisms into the realm of scientific inquiry, moving away from explanations requiring an “Intelligent Designer”.\nThis “trial and error” process, where successful variations are retained and propagated, has been analogously applied to the evolution of artifacts and designs. Modern architectural design is now explicitly leveraging this analogy through “genetic algorithms” and other computational methods to emulate natural evolution for form generation and optimization. These algorithms can generate numerous adaptive solutions and allow for dynamic evaluation criteria, accelerating the design process by rapidly testing and refining designs based on predefined parameters.\nHowever, a critical distinction exists between natural and artificial evolution in design: the “fitness function.” Natural selection in biology operates on a complex, multi-faceted concept of “fitness” within a dynamic and often unpredictable environment, leading to emergent adaptations. This fitness is an outcome of intricate interactions with the environment and other organisms. In contrast, genetic algorithms in architecture, while inspired by natural evolution, require explicitly defined “evaluating criteria” or “optimization indexes”. These criteria might include quantifiable metrics such as minimizing steel cost, maximizing solar energy generation , or optimizing structural efficiency.\nThe disparity lies in the simplification of “fitness” in artificial systems. Real-world architectural “fitness” is not solely about structural efficiency or energy performance; it also encompasses intangible yet crucial aspects such as social equity, cultural resonance, long-term adaptability, human well-being, and ecological integration. A significant limitation in applying evolutionary principles to architectural design might be the oversimplification of this “fitness” concept. While computational evolution can optimize for defined parameters, capturing the full “biological fitness” of a building requires a much more holistic and complex understanding of its interactions with human and natural systems. This suggests that while artificial intelligence can accelerate design, human understanding and ethical considerations remain paramount in defining what constitutes a truly “fit” building in a complex societal and ecological context. IV. Methodologies of Understanding and Engineering: Tools, Data, and Philosophy\nThe methodologies employed in both biological research and architectural practice, encompassing data management, economic flows, and philosophical underpinnings, reveal shared challenges and unique disciplinary “blind spots.” Nomenclature and Standardization: Challenges and Regional Variations\nThe precise, yet often inconsistent, language used to describe components and processes poses a significant challenge to effective communication and collaboration in both biology and architecture. This issue is particularly pronounced in interdisciplinary contexts where shared understanding is paramount.\nIn biology, gene and protein nomenclature presents a “pernicious problem” [user query]. The field grapples with an assortment of alternate names, species-specific guidelines, and names that bear no clear relation to function or structure, a consequence of the historical, piecemeal discovery process. This fragmentation complicates the organization and exchange of biological information, leading to potential misunderstandings and hindering collaborative efforts. For instance, a single gene might encode multiple protein products, or a protein might have several synonymous names across different organisms or research groups.\nSimilarly, architectural terminology exhibits considerable regional variation and differences between countries. The anecdote regarding “summer beams” illustrates how historical quirks and local practices can lead to terms whose origins are obscure or regionally specific. Other examples of such terminological divergence include “bungalow” (UK) versus “ranch house” (US), or “skirting board” (UK) versus “baseboard” (US). Vernacular architecture, in particular, vividly demonstrates how local climate, culture, and available materials lead to diverse building forms and their associated, often localized, terminology. These regional variations, while culturally rich, can impede standardized communication in a globalized construction industry.\nThe challenge in both fields lies in the tension between historical legacy and the drive for global standardization. While standardization offers clear benefits for efficiency, data exchange, and reducing ambiguity, the historical and regional variations often embed rich cultural, practical, and environmental knowledge. Arbitrarily imposing new naming conventions can erase this historical and contextual richness, potentially alienating practitioners and obscuring valuable traditional knowledge. This highlights a limitation in the pursuit of purely rationalized, standardized systems. The inherent value of local, historically-derived knowledge, even if seemingly “quirky” or inconsistent, might be overlooked in favor of abstract, global classifications. Both fields must navigate this balance, perhaps through the development of interoperable ontologies or mapping tools that bridge different terminological systems rather than imposing a single, rigid standard. Applied Practice and Economic Flows\nThe practical execution of both architectural projects and biological research is profoundly shaped by economic realities, resource allocation, and logistical challenges. These external factors often dictate what is feasible, what is prioritized, and ultimately, what is built or discovered.\nArchitecture: In architecture and construction, building specifications are paramount, serving as the “rules of the game” for builders [user query]. These documents provide detailed descriptions of quality, standards, materials, and procedures, ensuring clear communication, reducing errors, and establishing consistent expectations among all stakeholders, from clients to contractors and suppliers. Construction projects follow a structured pipeline, typically involving initiation, planning, procurement (material acquisition), construction, and closeout stages. This process involves complex economic flows, from budgeting and cost control to managing labor and intricate material supply chains.\nHowever, the construction industry faces significant challenges, including material shortages, rising costs, labor shortages, and complex logistical issues. The North American housing crisis exemplifies how prevailing economic systems can create systemic limitations. The increasing financialization of real estate, where housing is viewed primarily as a financial asset for speculation rather than a fundamental human need for shelter, fuels speculation, consolidates market power, and leads to a severe lack of affordable housing and “unimaginative copypasted condominius”. This demonstrates a critical disconnect between financial models and societal needs in housing. The prevailing economic models in architecture and real estate often fail to prioritize fundamental human needs (shelter, sovereignty) and ecological well-being over short-term financial returns. This calls for a re-evaluation of the “purpose” of building within a broader societal context. Community land trusts (CLTs) offer a tangible alternative by separating the cost of land from the residence, promoting long-term affordability and community sovereignty through community-governed structures.\nBiology: Structural biology research relies on sophisticated and often expensive techniques like X-ray crystallography, Cryo-electron microscopy (Cryo-EM), and Nuclear Magnetic Resonance (NMR) spectroscopy. Genetic sequencing also involves significant costs. The funding mechanisms for biological research, primarily government grants (e.g., NIH, NSF, UKRI) and philanthropic trusts (e.g., Wellcome Trust), profoundly influence research priorities, infrastructure, and methodology. For instance, decisions like capping NIH grant overhead rates can lead to billions of dollars in losses for research institutions, jeopardizing critical infrastructure and staffing.\nBiological data management faces unique challenges, exemplified by the Protein Data Bank (PDB) format’s 80-character limit, a remnant of the punch card era that persisted until around 2014. This anecdote highlights how legacy data formats and historical inertia can inadvertently “bind” and constrain modern research practices, forcing workarounds like splitting larger files. This demonstrates that outdated technical infrastructure and historical inertia can impose significant, long-lasting constraints on scientific practice. Furthermore, funding decisions directly impact the financial viability of research institutions, affecting equipment and personnel. Ethical frameworks and data privacy regulations are not merely guidelines but actively shape what research can be conducted, how data is handled, and what research questions are deemed permissible or fundable. These seemingly external, non-scientific factors (legacy technology, funding policies, ethical/legal frameworks) are not passive background elements but actively “inform and steer and inadvertently bind biology” [user query]. They represent a significant area of oversight if researchers focus solely on scientific questions without understanding the broader socio-technical and economic ecosystem within which their work is embedded. This suggests that the “progress” of science is not solely driven by intellectual curiosity or experimental ingenuity, but is profoundly shaped by economic realities, political decisions, and historical technological legacies. Understanding these “invisible” forces is crucial for both optimizing scientific progress and addressing its broader societal implications and ethical responsibilities.\nGenetic engineering, while transformative, raises significant ethical dilemmas concerning biodiversity loss, increased corporate control over food supplies, potential animal suffering, eugenics, safety, access, and the commodification of human beings. The immense volume and sensitive nature of biological data (e.g., genetic information, health records) further necessitate stringent privacy and security measures, as misuse can lead to discrimination and identity theft.\nThe following table provides a comparative view of economic flows and resource allocation in both fields:\nTable 2: Economic Flows and Resource Allocation: A Cross-Disciplinary View Aspect of Economic Flow Architecture/Construction Biology/Research Shared Challenges/Parallels Primary Goal Shelter/Investment Knowledge/Discovery Balancing efficiency with societal good Funding Sources Private investment, Government subsidies Government grants (NIH, NSF, UKRI), Philanthropic trusts Budget constraints Key Cost Drivers Land/materials/labor costs Equipment (microscopes, sequencers)/personnel/data storage costs Supply chain dependencies Resource Acquisition/Supply Chain Complex supply chains, Local sourcing Lab supplies, Specialized reagents Legacy system inertia Production/Construction/Research Activities Project management, Building specifications, On-site construction Experimental design, Data collection, Data analysis Ethical dilemmas/public trust Data/Information Management Building Information Modeling (BIM), Blueprints Data repositories (PDB), Databases Impact of financialization/funding decisions Societal/Ethical Impact Housing crisis, Architectural exclusion, Community land trusts Genetic engineering ethics, Data privacy, Eugenics concerns Need for interdisciplinary collaboration The “Blind Spots” in Methodologies\nBoth architecture and biology, despite their rigorous methodologies, possess inherent “blind spots”—areas or aspects that are systematically overlooked, undervalued, or rendered invisible by prevailing paradigms, tools, or external pressures. Recognizing these limitations is crucial for advancing both fields.\nArchitecture: Architectural limitations often arise when “ephemeral (lasting) aesthetics and budgets take precedence over long lasting functionality”. This includes neglecting crucial aspects like human factors, long-term building safety, climate responsiveness, and true sustainability. Critiques highlight how architectural practice can become disconnected from broader public concerns, social justice, and the socio-economic realities of the communities it serves. For example, urban design elements can function as “architectural exclusion,” subtly making places inaccessible to certain groups, a “hidden power” that shapes behavior without explicit awareness. This is not merely a technical oversight but an ethical failure, stemming from a limited scope of responsibility or a narrow definition of “success.” This calls for a broader ethical framework in architectural education and practice that explicitly mandates consideration of the long-term societal, environmental, and political impacts of design decisions, moving beyond immediate client demands, aesthetic trends, or purely economic metrics.\nThere is also a fundamental “tension between what is predefined and what is experienced” in architectural space, where the dynamic, unpredictable reality of human use often “deforms” or overflows the static, designed form. This highlights a limitation in accounting for the lived experience post-occupancy, a critical aspect that traditional design methodologies may not fully capture.\nBiology: Biological research also contends with limitations that stem from the inherent properties of its methods, definitions, or theoretical approaches. For instance, certain research methods may be excellent for quantitative data but less effective at probing underlying reasons or complex causal relationships. The increasing reliance on high-throughput data and computational methods, such as machine learning models like AlphaFold for protein structure prediction or single-cell RNA sequencing for cell mapping , introduces new areas of oversight. These can relate to data bias (e.g., AlphaFold trained on existing PDB structures, which might have inherent biases) , model limitations, or the inherent interpretability challenges of complex algorithms. While these tools offer unprecedented resolution, they can also obscure nuances or create a “digital” limitation if not critically assessed.\nThe philosophical debate between reductionism and holism in biology highlights a core methodological tension: focusing too deeply on sub-angstrom scales might inadvertently miss the emergent properties of the whole cell or organism. The concept of the cell itself has evolved from simple structural units to a “dynamic network,” reflecting a move towards a more holistic, systems-level understanding. The epistemological limitation of data-driven reductionism is a significant concern. Modern biology’s increasing reliance on high-throughput data and computational models to “deconstruct” existing biological systems leads to profound philosophical questions about “cell identity mapping”. While powerful, this data-driven approach carries risks of “descriptor bias,” “averaging out heterogeneities” in bulk measurements, and potential oversimplification. The “physiological blind spot” in human vision serves as a potent analogy: our sophisticated tools, while revealing much, inherently limit what we can perceive or interpret. This potential for data-driven reductionism to miss holistic, emergent properties or to oversimplify complex biological realities, especially if models are trained on existing (and potentially incomplete or biased) experimental data, is a critical area of oversight. This suggests a crucial need for philosophical self-reflection within biology regarding the inherent limitations of its tools and data-driven approaches. It emphasizes that “mapping more and more rooms and tenants” [user query] does not automatically equate to full understanding, especially if the “map” itself has inherent biases or limits what can be seen. This points to a need for more integrative structural biology and systems biology approaches that explicitly seek to bridge scales and integrate diverse data types for a more comprehensive understanding.\nConclusion\nThe exploration of connections between biology and architecture reveals a rich tapestry of shared principles, analogous processes, and common challenges. From the hierarchical organization that defines both a living cell and a city block, to the elegant efficiency of tensegrity structures found in both biological and built forms, the underlying “design principles” often transcend disciplinary boundaries. The evolution of both life and human engineering, whether through unconscious adaptation or deliberate biomimicry, highlights a continuous quest for optimal solutions in dynamic environments.\nHowever, this interdisciplinary lens also illuminates critical “blind spots” within each field. In architecture, the prevailing mercantile system often prioritizes short-term financial gains and aesthetics over long-term functionality, social equity, and environmental sustainability, leading to a housing crisis and a disconnect from fundamental human needs. Similarly, in biological research, the reliance on advanced, expensive technologies and the influence of funding mechanisms, coupled with historical data format legacies, can inadvertently shape research priorities and limit the scope of inquiry. The philosophical tension between reductionism and holism, and the potential for data-driven approaches to oversimplify complex biological realities, underscores the need for continuous methodological self-reflection.\nUltimately, both biology and architecture grapple with the profound human tendency to map and subdivide the “problem” of spatial theaters—the ones we inhabit and the ones that constitute us. This cognitive imperative, while a powerful engine for innovation, also introduces inherent limitations. Recognizing these shared challenges and limitations is not a sign of weakness but an opportunity for growth. By fostering deeper interdisciplinary dialogue, embracing systems-level thinking, and critically examining the socio-economic and ethical contexts that shape their practices, both structural biologists and architects can move towards more holistic, adaptive, and responsible approaches to understanding and shaping the complex structures of our world. The lessons learned from the dynamic, self-organizing nature of life offer profound insights for building more resilient, sustainable, and human-centric environments.\n\nTensegrity:\nExamples in architecture: - roofs of the Spodek arena complex, the Seoul Olympic Gymnastics Arena, the Georgia Dome, and Tropicana Field, as well as the Kurilpa Bridge - tarp, fly-tent\nExamples in biology: - human skeletal structure - the tension in the cell - tubulin networks as beams\nSome examples of ingenuity and discovery in architectural history: ex how a new type of arch or beam was discovered, the part of the identity of the discoverer[s] and the context of the discovery that was instrumental to it.\nI guess a big motivating factor in trying to see how much one can apprehend/one-shot about a field this way is the question of total reorganization. That is, if accept that “things have never been so ripe for a change” – then how can a contour of that change look like, what is a better organization? Not from a place of radicalism or vanity, but from a place of making use of the insight and tools that have become available since the long-ago birth or creation of the systems and notions that still rule the day.\n“Ultimately, both biology and architecture grapple with the profound human tendency to map and subdivide the”problem” of spatial theaters—the ones we inhabit and the ones that constitute us. This cognitive imperative, while a powerful engine for innovation, also introduces inherent limitation”\nIt is crucial to recognize this tendency as just that, a tendency. At its most general, - it is not a malicious device to disempower the proletariat by real estate cartels and investing groups and realtors and corporations - it is not an outcome of a few misguided low interest rates policies 15 years ago - it is not “the immigrants” that make it complicated - it is not a lack of materials\nAll of the above are just manifestations of the human tendency to divide and conquer in other fields and strata of the economy on the craft of the builder and the architect. It seems like one defense against these assaults is to strengthen a vision and a holistic and total understanding of how architecture interacts with these other fields. That is, the solution to housing under assault is not to cure one or two of the political problems and keep building like before, but to upgrade and equip the discipline itself to be able to put an answer to the modern problems. To the architect and the builder, it seems, the relief from without should not be counted on and must come from the strengthening and sophistication within.\nSame for biology. It is not like there is a “crisis of imaging” in biology but i feel like the time ripe for another scaffold around around the old one. What we view now as the limits of our discipline must become the formwork surface supporting what ought to be. The virtual cell project, cryoEt, ML and Qchem folding models all warrant greater infrastructure. The immense computational capacities we posses make the current organisation of knowledge, the “rails” on which this train might run, seem ridiculous, stunted and limited.\n\nArchitectural/Construction Terms:\nScaffolding & Formwork:\n\nFormwork - temporary structure that holds concrete until it sets (very close to your concept)\nFalsework - temporary framework supporting a structure during construction\nCentering - temporary wooden framework for building arches/vaults\nShoring - temporary supports for existing structures\n\nThe Process:\n\nIncremental construction or staged construction\nNested formwork or recursive scaffolding\nTelescoping construction (like extending telescope tubes)\n\nBiological Analogies:\nCellular Processes:\n\nCytoskeleton - internal scaffolding of cells that gets reorganized as cells grow\nExtracellular matrix (ECM) - scaffolding that cells grow on and remodel\nEndosymbiosis - cells engulfing other cells, creating nested structures\nTissue engineering scaffolds - temporary frameworks that guide new tissue growth\n\nGrowth Patterns:\n\nCoral accretion - new polyps build on existing calcium carbonate scaffolds\nTree rings - each growth season creates a new layer around the previous structure\nMollusk shell growth - spiral growth adding chambers (like nautilus shells)\n\nKey Components:\n\nPrimary scaffold/framework - your initial structure\nInfill material - what grows within and beyond the scaffold\nSecondary containment - the larger scaffold that accommodates overflow\nGrowth nodes - points where material \"pokes through\"\nTransition zones - areas where one scaffold hands off to the next"
  },
  {
    "objectID": "listings/beautiful_code.html",
    "href": "listings/beautiful_code.html",
    "title": "артём",
    "section": "",
    "text": "Ian W Davis - FoldTree"
  },
  {
    "objectID": "WIP_posts/mmcif_stuff/report1.html",
    "href": "WIP_posts/mmcif_stuff/report1.html",
    "title": "Structural Biology’s Data Format Crisis: Why PDB/mmCIF Can’t Keep Pace with the AI Era",
    "section": "",
    "text": "The structural biology field faces an inflection point where its foundational data formats—designed for punch cards in 1972—cannot adequately represent AI-generated ensembles, petabyte-scale cryo-ET datasets, or dynamic molecular behavior. The 99,999 atom limit, parser fragmentation across dozens of incompatible implementations, and inability to encode conformational heterogeneity are blocking breakthrough applications. Meanwhile, other fields have solved analogous problems: genomics achieved interoperability through HTSlib and GA4GH governance, while microscopy embraced cloud-native OME-Zarr. Structural biology must adopt similar strategies—chunked cloud formats, ensemble-centric representations, and coordinated community governance—or risk becoming an isolated island in the multi-omics landscape.\n\n\nThe PDB format’s 80-column punch-card layout imposes hard constraints that break modern workflows. The maximum 99,999 atoms (5-character field) means large cryo-EM structures display asterisks (*****) that crash parsers. The 62-chain limit (single-character identifier) forces “split entries” where ribosomes were historically divided across multiple PDB IDs. Residue numbers cap at 9,999 per chain. These aren’t theoretical concerns—MDAnalysis GitHub issues document parsing failures when atom numbers overflow, and the BeEM converter paper notes that BioPython, cif-tools, and Atomium handle only single-character chain IDs.\nEven mmCIF, designed to solve these limitations, creates new problems. The format contains two competing numbering systems—auth_seq_id (author-provided) versus label_seq_id (PDB-assigned)—that cause constant confusion. Chimera mailing lists document users puzzled when the same structure shows different residue numbers depending on which field the viewer reads. PyMOL defaults to auth_* fields while other tools use label_*, creating incompatible coordinate references for the same molecule.\nParser fragmentation is severe. The project-gemmi/mmcif-benchmark reveals dramatic performance differences: reading a 230MB file takes 7.8 seconds in Gemmi but causes out-of-memory crashes in BioPython and iotbx-pdb. BioPython itself maintains two separate parsers—MMCIFParser and FastMMCIFParser—because the “correct” parser is too slow while the fast one “doesn’t aim to parse mmCIF correctly.” GitHub issues across BioPython (#775, #481, #778, #1206), MDAnalysis (#446, #2422, #3473), Gemmi (#24, #118, #178), and Boltz (#451) document parsing failures ranging from missing atom serial numbers to crashes on alternate conformations to files generated by one tool being unreadable by another.\n\n\n\nThe data volume explosion is staggering. EMPIAR exceeds 2 petabytes with individual datasets reaching 70+ terabytes—taking weeks to transfer even with high-bandwidth connections. The Chan Zuckerberg CryoET Data Portal contains over 16,000 annotated tomograms, but annotations exist in “a wide variety of formats with varying forms and completeness of metadata,” making algorithm development and data reuse prohibitively difficult. A joint EBI/CZ Imaging Institute working group formed in April 2024 specifically to address this standardization gap.\nAlphaFold’s 214 million predicted structures introduced new data types that don’t fit existing formats. The pLDDT confidence score gets repurposed into the B-factor field—but unlike B-factors, higher pLDDT is better, causing confusion in molecular replacement workflows. The Predicted Aligned Error (PAE) matrix, critical for multi-domain interpretation, requires separate JSON files with custom parsing. Multiple sequence alignments (.a3m files) essential for reproducibility exist outside the coordinate format entirely.\nXFEL serial crystallography generates terabytes per hour, scaling to petabytes per day at next-generation facilities. European XFEL’s 1.1 MHz repetition rate produces hundreds of thousands of diffraction patterns hourly from crystals in random unknown orientations. Multi-panel detector geometries require complex non-standardized geometry files. Real-time processing must occur during collection—current formats weren’t designed for streaming data.\n\n\n\nStructural data lacks biological meaning. A critical finding: over 60% of proteins assigned enzymatic function in SwissProt have no active site residues identified in structural databases. The Catalytic Site Atlas covers only one reference structure per curated function and isn’t regularly updated. Mapping a variant from ClinVar to its structural context requires navigating multiple databases with incompatible identifiers.\nBiological assembly annotations are surprisingly unreliable. Estimates suggest 10-15% of PDB entries contain incorrect or ambiguous biological assembly annotations. PISA algorithms often fail to correctly score heteromeric assemblies, particularly with small subunits. Crystal packing contacts get confused with biological interfaces. There’s no standardized naming convention, complicating computational analysis.\nThe wwPDB launched a major PTM remediation project (October 2024 – Spring 2025) acknowledging that post-translational modifications were historically handled inconsistently. A new pdbx_modification_feature category will provide instance-level PTM annotation, but this represents catching up rather than leading. Subcellular localization isn’t captured at all. Pathway context requires external database navigation. Proteoform-specific annotations are largely absent.\nSIFTS (Structure Integration with Function, Taxonomy and Sequences) provides the crucial bridge, mapping PDB residues to UniProtKB sequences weekly. Recent advances embedded SIFTS annotations directly into mmCIF files and expanded coverage 40-fold to 1.8+ million UniProtKB sequences through UniRef90. The PDBe knowledge graph contains over 1 billion nodes and 1.5 billion edges integrating 30+ partner resources. But these remain external layers rather than intrinsic to the data format.\n\n\n\nHTSlib’s success offers a template. Downloaded over 1 million times and used by 900+ GitHub projects, this reference implementation made BAM/CRAM/VCF formats practical. Key design decisions: simple human-readable text format (SAM) paired with efficient binary (BAM), strong bundled reference implementation, indexability for random region access, extensibility via optional tags, and MIT/BSD licensing enabling commercial adoption. The format evolved—CRAM achieved 40-60% smaller files through reference-based compression while maintaining backward compatibility.\nThe GA4GH governance model provides organizational structure. Eight Work Streams and 24 Driver Projects engage over 1,000 individuals from 90+ countries and 650+ organizations. Standards emerge from real implementer needs through Study Groups, get formalized in Work Streams, and pilot through the Global Implementers Forum. This creates buy-in and ensures practical utility.\nOME-Zarr demonstrates cloud-native design done right. Built for object storage from inception, it uses chunked N-dimensional arrays where each chunk is an independent file. Multi-resolution pyramids are built in. Hierarchical JSON metadata sits at each level. Implementations exist in Python, Java, JavaScript, C++, Rust, and Julia. The sharding innovation in Zarr v3 groups multiple chunks per object to handle filesystem limits while maintaining parallelism. Latency advantages are fundamental: monolithic formats require multiple round-trips while Zarr chunks are independent.\nThe critical lesson: reference implementation quality determines adoption. Formats without good libraries don’t get used regardless of theoretical elegance. Community governance through neutral international consortia builds trust. Cloud-native chunked formats are essential for object storage. Extensibility via optional fields enables evolution without breaking existing tools.\n\n\n\nA 2024 paper in Acta Crystallographica D states the core problem directly: “Although new tools are available to detect conformational and compositional heterogeneity within these ensembles, the legacy PDB data structure does not robustly encapsulate this complexity.” Current formats encode single conformational states despite biomolecules existing in dynamic ensembles. Intrinsically disordered proteins populate conformational ranges best described by heterogeneous ensembles—features “notoriously difficult to characterize” because they’re “lost by ensemble methods of structural characterization.”\nAI-native format requirements are now well-understood: - E(3)/SE(3) equivariance preservation under geometric transformations - Native graph representations with k-nearest neighbor graphs, edge type annotations, and node features for GNNs like GearNet and DeepRank-GNN - Multi-resolution tokenization (AlphaFold 3 uses flexible schemes: standard residues as single tokens, modified residues as atoms, ligands as individual atoms) - Pre-computed MSA embeddings as feature channels - Confidence metrics (pLDDT, PAE) as first-class data alongside coordinates - Versioned model parameters linking predictions to the networks that generated them\nReal-time streaming for time-resolved experiments requires time-indexed 4D coordinate trajectories, native support for incomplete/sparse datasets, and streaming formats compatible with Apache Kafka/Spark. BioCARS achieves time resolutions from 100 picoseconds to seconds; mix-and-inject serial crystallography captures enzyme catalysis with 2-7 millisecond temporal resolution. These generate data requiring storage paradigms beyond static files.\nEnsemble representation extensions should include conformer population weights as first-class metadata, state transition matrices linking related conformations, per-coordinate uncertainty quantification, and links between experimental observables and ensemble statistics. The Protein Ensemble Database (PED) demonstrates what structured IDP ensemble metadata looks like—conformer counts, modeling resolution levels, validation status—but this remains isolated from mainstream structural formats.\n\n\n\nThe disconnect is stark: each omics type has distinct data structures, distributions, and batch effects. There are no standardized preprocessing protocols across omics layers. Programs require non-standard inputs and output incompatible formats, forcing researchers to write conversion scripts for every integration.\nTools like GLUE use graph variational autoencoders to anchor features using prior biological knowledge. MOFA+ and iCluster map multi-omics onto shared representations. But structural data sits outside these frameworks. A complete structural-omics pipeline—raw cryo-EM through single-particle analysis, subunit fitting, mass spectrometry validation, multi-omics annotation, integrative modeling via IMP or HADDOCK, to validated assembly—currently requires crossing multiple format boundaries with manual intervention at each step.\nThe PDBe knowledge graph represents the most ambitious current integration, but accessing biological context still requires navigating multiple APIs and external resources. For structural data to participate in systems biology, format-level integration points must exist: knowledge graphs linking structural data to genomic variants (AlphaMissense), protein modifications, interaction networks, and pathway annotations as native data channels rather than external lookups.\n\n\n\nNear-term priorities (1-5 years) should include mandatory PDBx/mmCIF adoption before four-character PDB IDs exhaust around 2028, standardized cryo-EM interchange eliminating the cryoSPARC/RELION conversion mess, ensemble mmCIF extensions implementing proposed conformational heterogeneity categories, and GraphML/JSON-Graph exports providing ML-ready graph representations from coordinate files.\nMedium-term developments (5-15 years) require streaming data standards with time-indexed 4D structural formats, multi-omics structural schemas linking to genomic/proteomic annotations, federated structural databases enabling distributed queries across PDB/EMDB/BMRB/PED, and AI model versioning tracking which networks produced which predictions.\nCloud infrastructure exists but needs structural biology adaptation. SBGrid’s SBCloud provides Slurm-based clusters with 620+ curated structural biology applications through Open OnDemand browser interfaces. Provenance tracking exists in specialized systems like PDB-REDO Cloud with detailed records documenting input versioning and program versions. The technical foundations are present—PDBx/mmCIF extensibility, cloud infrastructure, graph neural network frameworks, FAIR principles. The challenge is coordinated community adoption.\n\n\n\nThe wwPDB PDBx/mmCIF Working Group faces a forcing function: the 2028 extended PDB ID transition will require universal format migration regardless. This creates an opportunity to implement ensemble representation extensions, AI-native features, and cloud-optimized chunked storage simultaneously rather than incrementally. The alternative—continuing with formats designed for punch cards while the rest of biology moves to cloud-native, AI-ready infrastructure—risks structural biology becoming isolated from the integrated multi-omics future. Other fields have demonstrated the path: strong reference implementations, neutral governance bodies, extensible formats, and cloud-native architecture. The question is whether structural biology will follow before the format limitations become the binding constraint on discovery."
  },
  {
    "objectID": "WIP_posts/mmcif_stuff/report1.html#the-legacy-burden-why-everyone-writes-their-own-parser",
    "href": "WIP_posts/mmcif_stuff/report1.html#the-legacy-burden-why-everyone-writes-their-own-parser",
    "title": "Structural Biology’s Data Format Crisis: Why PDB/mmCIF Can’t Keep Pace with the AI Era",
    "section": "",
    "text": "The PDB format’s 80-column punch-card layout imposes hard constraints that break modern workflows. The maximum 99,999 atoms (5-character field) means large cryo-EM structures display asterisks (*****) that crash parsers. The 62-chain limit (single-character identifier) forces “split entries” where ribosomes were historically divided across multiple PDB IDs. Residue numbers cap at 9,999 per chain. These aren’t theoretical concerns—MDAnalysis GitHub issues document parsing failures when atom numbers overflow, and the BeEM converter paper notes that BioPython, cif-tools, and Atomium handle only single-character chain IDs.\nEven mmCIF, designed to solve these limitations, creates new problems. The format contains two competing numbering systems—auth_seq_id (author-provided) versus label_seq_id (PDB-assigned)—that cause constant confusion. Chimera mailing lists document users puzzled when the same structure shows different residue numbers depending on which field the viewer reads. PyMOL defaults to auth_* fields while other tools use label_*, creating incompatible coordinate references for the same molecule.\nParser fragmentation is severe. The project-gemmi/mmcif-benchmark reveals dramatic performance differences: reading a 230MB file takes 7.8 seconds in Gemmi but causes out-of-memory crashes in BioPython and iotbx-pdb. BioPython itself maintains two separate parsers—MMCIFParser and FastMMCIFParser—because the “correct” parser is too slow while the fast one “doesn’t aim to parse mmCIF correctly.” GitHub issues across BioPython (#775, #481, #778, #1206), MDAnalysis (#446, #2422, #3473), Gemmi (#24, #118, #178), and Boltz (#451) document parsing failures ranging from missing atom serial numbers to crashes on alternate conformations to files generated by one tool being unreadable by another."
  },
  {
    "objectID": "WIP_posts/mmcif_stuff/report1.html#cryoet-and-alphafold-strain-every-assumption",
    "href": "WIP_posts/mmcif_stuff/report1.html#cryoet-and-alphafold-strain-every-assumption",
    "title": "Structural Biology’s Data Format Crisis: Why PDB/mmCIF Can’t Keep Pace with the AI Era",
    "section": "",
    "text": "The data volume explosion is staggering. EMPIAR exceeds 2 petabytes with individual datasets reaching 70+ terabytes—taking weeks to transfer even with high-bandwidth connections. The Chan Zuckerberg CryoET Data Portal contains over 16,000 annotated tomograms, but annotations exist in “a wide variety of formats with varying forms and completeness of metadata,” making algorithm development and data reuse prohibitively difficult. A joint EBI/CZ Imaging Institute working group formed in April 2024 specifically to address this standardization gap.\nAlphaFold’s 214 million predicted structures introduced new data types that don’t fit existing formats. The pLDDT confidence score gets repurposed into the B-factor field—but unlike B-factors, higher pLDDT is better, causing confusion in molecular replacement workflows. The Predicted Aligned Error (PAE) matrix, critical for multi-domain interpretation, requires separate JSON files with custom parsing. Multiple sequence alignments (.a3m files) essential for reproducibility exist outside the coordinate format entirely.\nXFEL serial crystallography generates terabytes per hour, scaling to petabytes per day at next-generation facilities. European XFEL’s 1.1 MHz repetition rate produces hundreds of thousands of diffraction patterns hourly from crystals in random unknown orientations. Multi-panel detector geometries require complex non-standardized geometry files. Real-time processing must occur during collection—current formats weren’t designed for streaming data."
  },
  {
    "objectID": "WIP_posts/mmcif_stuff/report1.html#the-biological-context-chasm-between-structure-and-function",
    "href": "WIP_posts/mmcif_stuff/report1.html#the-biological-context-chasm-between-structure-and-function",
    "title": "Structural Biology’s Data Format Crisis: Why PDB/mmCIF Can’t Keep Pace with the AI Era",
    "section": "",
    "text": "Structural data lacks biological meaning. A critical finding: over 60% of proteins assigned enzymatic function in SwissProt have no active site residues identified in structural databases. The Catalytic Site Atlas covers only one reference structure per curated function and isn’t regularly updated. Mapping a variant from ClinVar to its structural context requires navigating multiple databases with incompatible identifiers.\nBiological assembly annotations are surprisingly unreliable. Estimates suggest 10-15% of PDB entries contain incorrect or ambiguous biological assembly annotations. PISA algorithms often fail to correctly score heteromeric assemblies, particularly with small subunits. Crystal packing contacts get confused with biological interfaces. There’s no standardized naming convention, complicating computational analysis.\nThe wwPDB launched a major PTM remediation project (October 2024 – Spring 2025) acknowledging that post-translational modifications were historically handled inconsistently. A new pdbx_modification_feature category will provide instance-level PTM annotation, but this represents catching up rather than leading. Subcellular localization isn’t captured at all. Pathway context requires external database navigation. Proteoform-specific annotations are largely absent.\nSIFTS (Structure Integration with Function, Taxonomy and Sequences) provides the crucial bridge, mapping PDB residues to UniProtKB sequences weekly. Recent advances embedded SIFTS annotations directly into mmCIF files and expanded coverage 40-fold to 1.8+ million UniProtKB sequences through UniRef90. The PDBe knowledge graph contains over 1 billion nodes and 1.5 billion edges integrating 30+ partner resources. But these remain external layers rather than intrinsic to the data format."
  },
  {
    "objectID": "WIP_posts/mmcif_stuff/report1.html#how-genomics-and-imaging-solved-similar-problems",
    "href": "WIP_posts/mmcif_stuff/report1.html#how-genomics-and-imaging-solved-similar-problems",
    "title": "Structural Biology’s Data Format Crisis: Why PDB/mmCIF Can’t Keep Pace with the AI Era",
    "section": "",
    "text": "HTSlib’s success offers a template. Downloaded over 1 million times and used by 900+ GitHub projects, this reference implementation made BAM/CRAM/VCF formats practical. Key design decisions: simple human-readable text format (SAM) paired with efficient binary (BAM), strong bundled reference implementation, indexability for random region access, extensibility via optional tags, and MIT/BSD licensing enabling commercial adoption. The format evolved—CRAM achieved 40-60% smaller files through reference-based compression while maintaining backward compatibility.\nThe GA4GH governance model provides organizational structure. Eight Work Streams and 24 Driver Projects engage over 1,000 individuals from 90+ countries and 650+ organizations. Standards emerge from real implementer needs through Study Groups, get formalized in Work Streams, and pilot through the Global Implementers Forum. This creates buy-in and ensures practical utility.\nOME-Zarr demonstrates cloud-native design done right. Built for object storage from inception, it uses chunked N-dimensional arrays where each chunk is an independent file. Multi-resolution pyramids are built in. Hierarchical JSON metadata sits at each level. Implementations exist in Python, Java, JavaScript, C++, Rust, and Julia. The sharding innovation in Zarr v3 groups multiple chunks per object to handle filesystem limits while maintaining parallelism. Latency advantages are fundamental: monolithic formats require multiple round-trips while Zarr chunks are independent.\nThe critical lesson: reference implementation quality determines adoption. Formats without good libraries don’t get used regardless of theoretical elegance. Community governance through neutral international consortia builds trust. Cloud-native chunked formats are essential for object storage. Extensibility via optional fields enables evolution without breaking existing tools."
  },
  {
    "objectID": "WIP_posts/mmcif_stuff/report1.html#what-would-transform-the-field-ensemble-native-ai-ready-infrastructure",
    "href": "WIP_posts/mmcif_stuff/report1.html#what-would-transform-the-field-ensemble-native-ai-ready-infrastructure",
    "title": "Structural Biology’s Data Format Crisis: Why PDB/mmCIF Can’t Keep Pace with the AI Era",
    "section": "",
    "text": "A 2024 paper in Acta Crystallographica D states the core problem directly: “Although new tools are available to detect conformational and compositional heterogeneity within these ensembles, the legacy PDB data structure does not robustly encapsulate this complexity.” Current formats encode single conformational states despite biomolecules existing in dynamic ensembles. Intrinsically disordered proteins populate conformational ranges best described by heterogeneous ensembles—features “notoriously difficult to characterize” because they’re “lost by ensemble methods of structural characterization.”\nAI-native format requirements are now well-understood: - E(3)/SE(3) equivariance preservation under geometric transformations - Native graph representations with k-nearest neighbor graphs, edge type annotations, and node features for GNNs like GearNet and DeepRank-GNN - Multi-resolution tokenization (AlphaFold 3 uses flexible schemes: standard residues as single tokens, modified residues as atoms, ligands as individual atoms) - Pre-computed MSA embeddings as feature channels - Confidence metrics (pLDDT, PAE) as first-class data alongside coordinates - Versioned model parameters linking predictions to the networks that generated them\nReal-time streaming for time-resolved experiments requires time-indexed 4D coordinate trajectories, native support for incomplete/sparse datasets, and streaming formats compatible with Apache Kafka/Spark. BioCARS achieves time resolutions from 100 picoseconds to seconds; mix-and-inject serial crystallography captures enzyme catalysis with 2-7 millisecond temporal resolution. These generate data requiring storage paradigms beyond static files.\nEnsemble representation extensions should include conformer population weights as first-class metadata, state transition matrices linking related conformations, per-coordinate uncertainty quantification, and links between experimental observables and ensemble statistics. The Protein Ensemble Database (PED) demonstrates what structured IDP ensemble metadata looks like—conformer counts, modeling resolution levels, validation status—but this remains isolated from mainstream structural formats."
  },
  {
    "objectID": "WIP_posts/mmcif_stuff/report1.html#the-multi-omics-integration-imperative",
    "href": "WIP_posts/mmcif_stuff/report1.html#the-multi-omics-integration-imperative",
    "title": "Structural Biology’s Data Format Crisis: Why PDB/mmCIF Can’t Keep Pace with the AI Era",
    "section": "",
    "text": "The disconnect is stark: each omics type has distinct data structures, distributions, and batch effects. There are no standardized preprocessing protocols across omics layers. Programs require non-standard inputs and output incompatible formats, forcing researchers to write conversion scripts for every integration.\nTools like GLUE use graph variational autoencoders to anchor features using prior biological knowledge. MOFA+ and iCluster map multi-omics onto shared representations. But structural data sits outside these frameworks. A complete structural-omics pipeline—raw cryo-EM through single-particle analysis, subunit fitting, mass spectrometry validation, multi-omics annotation, integrative modeling via IMP or HADDOCK, to validated assembly—currently requires crossing multiple format boundaries with manual intervention at each step.\nThe PDBe knowledge graph represents the most ambitious current integration, but accessing biological context still requires navigating multiple APIs and external resources. For structural data to participate in systems biology, format-level integration points must exist: knowledge graphs linking structural data to genomic variants (AlphaMissense), protein modifications, interaction networks, and pathway annotations as native data channels rather than external lookups."
  },
  {
    "objectID": "WIP_posts/mmcif_stuff/report1.html#toward-next-generation-structural-data-infrastructure",
    "href": "WIP_posts/mmcif_stuff/report1.html#toward-next-generation-structural-data-infrastructure",
    "title": "Structural Biology’s Data Format Crisis: Why PDB/mmCIF Can’t Keep Pace with the AI Era",
    "section": "",
    "text": "Near-term priorities (1-5 years) should include mandatory PDBx/mmCIF adoption before four-character PDB IDs exhaust around 2028, standardized cryo-EM interchange eliminating the cryoSPARC/RELION conversion mess, ensemble mmCIF extensions implementing proposed conformational heterogeneity categories, and GraphML/JSON-Graph exports providing ML-ready graph representations from coordinate files.\nMedium-term developments (5-15 years) require streaming data standards with time-indexed 4D structural formats, multi-omics structural schemas linking to genomic/proteomic annotations, federated structural databases enabling distributed queries across PDB/EMDB/BMRB/PED, and AI model versioning tracking which networks produced which predictions.\nCloud infrastructure exists but needs structural biology adaptation. SBGrid’s SBCloud provides Slurm-based clusters with 620+ curated structural biology applications through Open OnDemand browser interfaces. Provenance tracking exists in specialized systems like PDB-REDO Cloud with detailed records documenting input versioning and program versions. The technical foundations are present—PDBx/mmCIF extensibility, cloud infrastructure, graph neural network frameworks, FAIR principles. The challenge is coordinated community adoption."
  },
  {
    "objectID": "WIP_posts/mmcif_stuff/report1.html#conclusion",
    "href": "WIP_posts/mmcif_stuff/report1.html#conclusion",
    "title": "Structural Biology’s Data Format Crisis: Why PDB/mmCIF Can’t Keep Pace with the AI Era",
    "section": "",
    "text": "The wwPDB PDBx/mmCIF Working Group faces a forcing function: the 2028 extended PDB ID transition will require universal format migration regardless. This creates an opportunity to implement ensemble representation extensions, AI-native features, and cloud-optimized chunked storage simultaneously rather than incrementally. The alternative—continuing with formats designed for punch cards while the rest of biology moves to cloud-native, AI-ready infrastructure—risks structural biology becoming isolated from the integrated multi-omics future. Other fields have demonstrated the path: strong reference implementations, neutral governance bodies, extensible formats, and cloud-native architecture. The question is whether structural biology will follow before the format limitations become the binding constraint on discovery."
  },
  {
    "objectID": "WIP_posts/tubulin/tubulin_notes/presentation.html",
    "href": "WIP_posts/tubulin/tubulin_notes/presentation.html",
    "title": "Why design a tool for specific macromolecule?",
    "section": "",
    "text": "Uniprot, Pfam are “flat” structures vis-a-vis the atomic models. There is value in building a layer in between.\n\nwant to centralize computations around the data, but not reinvent other tools. whati s the right balance?\ndata sovereignty within the community"
  },
  {
    "objectID": "WIP_posts/tubulin/tubulin_notes/presentation.html#tubulin",
    "href": "WIP_posts/tubulin/tubulin_notes/presentation.html#tubulin",
    "title": "Why design a tool for specific macromolecule?",
    "section": "Tubulin:",
    "text": "Tubulin:\n\nlibraries of cleaned-up ligands ready to go (via rosetta)\n..\n.."
  },
  {
    "objectID": "WIP_posts/tubulin/index.html",
    "href": "WIP_posts/tubulin/index.html",
    "title": "Tubulin Database Application Design",
    "section": "",
    "text": "From the perspective of the tubulin research community, here are the key elements your database application should incorporate:\n\n\n\nIsotype tracking: Alpha/beta/gamma tubulin variants with their distinct structural features\nPTM mapping: Comprehensive post-translational modification sites (acetylation, tyrosination, polyglutamylation, etc.)\nConformational states: Straight vs. curved, GTP vs. GDP-bound states\nLattice arrangements: A vs. B lattice, protofilament number variations (13 vs. non-13)\nInterface classifications: Lateral and longitudinal contact regions\nSeam structures: Variations in microtubule seam architecture\n\n\n\n\n\nPTM pattern search: Query by specific modification patterns\nStructural alignment tools: Compare conformational changes between states\nPharmacological binding site analysis: Drug interaction surfaces\nMAP binding region identification: Sites for motor proteins and other MAPs\nEvolutionary conservation mapping: Highlight conserved regions across species\nDisease-associated mutation correlation: Link mutations to structural changes\n\n\n\n\n\nNucleotide binding pockets: GTP/GDP binding sites\nTaxane-binding site: And other drug-binding domains\nE-site/M-site: Exchangeable vs. non-exchangeable nucleotide sites\nC-terminal tails: Highly variable regions subject to most PTMs\nInter-dimer interfaces: Critical for polymerization dynamics\nLoop regions: T5, H1-S2, M, etc. that change conformation during dynamics\n\n\n\n\n\nDynamics prediction: Tools to correlate structure with dynamic behavior\nResolution-based filtering: Quality assessment of structural data\nExperimental method classification: Cryo-EM vs. X-ray crystallography sources\nTaxonomic organization: Species-specific tubulin structures\nIntegrated visualization: Direct linking to molecular viewers with highlighting options\n\nWould you like me to expand on any particular aspect of this database design?"
  },
  {
    "objectID": "WIP_posts/tubulin/index.html#structural-classifications",
    "href": "WIP_posts/tubulin/index.html#structural-classifications",
    "title": "Tubulin Database Application Design",
    "section": "",
    "text": "Isotype tracking: Alpha/beta/gamma tubulin variants with their distinct structural features\nPTM mapping: Comprehensive post-translational modification sites (acetylation, tyrosination, polyglutamylation, etc.)\nConformational states: Straight vs. curved, GTP vs. GDP-bound states\nLattice arrangements: A vs. B lattice, protofilament number variations (13 vs. non-13)\nInterface classifications: Lateral and longitudinal contact regions\nSeam structures: Variations in microtubule seam architecture"
  },
  {
    "objectID": "WIP_posts/tubulin/index.html#biologically-relevant-operations",
    "href": "WIP_posts/tubulin/index.html#biologically-relevant-operations",
    "title": "Tubulin Database Application Design",
    "section": "",
    "text": "PTM pattern search: Query by specific modification patterns\nStructural alignment tools: Compare conformational changes between states\nPharmacological binding site analysis: Drug interaction surfaces\nMAP binding region identification: Sites for motor proteins and other MAPs\nEvolutionary conservation mapping: Highlight conserved regions across species\nDisease-associated mutation correlation: Link mutations to structural changes"
  },
  {
    "objectID": "WIP_posts/tubulin/index.html#critical-loci",
    "href": "WIP_posts/tubulin/index.html#critical-loci",
    "title": "Tubulin Database Application Design",
    "section": "",
    "text": "Nucleotide binding pockets: GTP/GDP binding sites\nTaxane-binding site: And other drug-binding domains\nE-site/M-site: Exchangeable vs. non-exchangeable nucleotide sites\nC-terminal tails: Highly variable regions subject to most PTMs\nInter-dimer interfaces: Critical for polymerization dynamics\nLoop regions: T5, H1-S2, M, etc. that change conformation during dynamics"
  },
  {
    "objectID": "WIP_posts/tubulin/index.html#additional-functionalities",
    "href": "WIP_posts/tubulin/index.html#additional-functionalities",
    "title": "Tubulin Database Application Design",
    "section": "",
    "text": "Dynamics prediction: Tools to correlate structure with dynamic behavior\nResolution-based filtering: Quality assessment of structural data\nExperimental method classification: Cryo-EM vs. X-ray crystallography sources\nTaxonomic organization: Species-specific tubulin structures\nIntegrated visualization: Direct linking to molecular viewers with highlighting options\n\nWould you like me to expand on any particular aspect of this database design?"
  },
  {
    "objectID": "WIP_posts/dotfiles/dotfiles.html",
    "href": "WIP_posts/dotfiles/dotfiles.html",
    "title": "xbindkeys",
    "section": "",
    "text": "xbindkeys\n\n\nclipboard\nIt’s clipmenud + rofi + xbindkeys for me.\nConfigure rofi to suit keyb needs . - CM_LAUNCHER='rofi' somewhere in zshrc, - In ~/.xbindkeysrc: \"clipmenu\"       Control+Alt + r # tmux"
  },
  {
    "objectID": "posts/npet-poisson-recon/index.html",
    "href": "posts/npet-poisson-recon/index.html",
    "title": "3D tessellation of biomolecular cavities",
    "section": "",
    "text": "We present a protocol to extract the surface of a biomolecular cavity for shape analysis and molecular simulations.\nWe apply and illustrate the protocol on the ribosome structure, which contains a subcompartment known as the ribosome exit tunnel or “nascent polypeptide exit tunnel” (NPET). More details on the tunnel features and biological importance can be found in our previous works (Dao Duc and Song 2018; Dao Duc et al. 2019).\nThe protocol was designed to refine the output obtained from MOLE software (Sehnal et al. 2013), but can be applied to reconstruct a mesh on any general point cloud. Hence, we take the point-cloud of atom positions surrounding the tunnel as a point of departure.\n\n\n\nIllustration of the ribosome exit tunnel (from Dao Duc et al., NAR 2019)\n\n\n\n\n\n\n\n\n\n\nSchematic representation of the protocol"
  },
  {
    "objectID": "posts/npet-poisson-recon/index.html#summary-and-background",
    "href": "posts/npet-poisson-recon/index.html#summary-and-background",
    "title": "3D tessellation of biomolecular cavities",
    "section": "",
    "text": "We present a protocol to extract the surface of a biomolecular cavity for shape analysis and molecular simulations.\nWe apply and illustrate the protocol on the ribosome structure, which contains a subcompartment known as the ribosome exit tunnel or “nascent polypeptide exit tunnel” (NPET). More details on the tunnel features and biological importance can be found in our previous works (Dao Duc and Song 2018; Dao Duc et al. 2019).\nThe protocol was designed to refine the output obtained from MOLE software (Sehnal et al. 2013), but can be applied to reconstruct a mesh on any general point cloud. Hence, we take the point-cloud of atom positions surrounding the tunnel as a point of departure.\n\n\n\nIllustration of the ribosome exit tunnel (from Dao Duc et al., NAR 2019)\n\n\n\n\n\n\n\n\n\n\nSchematic representation of the protocol"
  },
  {
    "objectID": "posts/npet-poisson-recon/index.html#pointcloud-preparation-bounding-box-and-voxelization",
    "href": "posts/npet-poisson-recon/index.html#pointcloud-preparation-bounding-box-and-voxelization",
    "title": "3D tessellation of biomolecular cavities",
    "section": "1. Pointcloud Preparation: Bounding Box and Voxelization",
    "text": "1. Pointcloud Preparation: Bounding Box and Voxelization\n\n\n\n\n\n\natompos_to_voxel_sphere: convert a 3D coordinate into a voxelized sphere\n\n\n\n\n\n\ndef atompos_to_voxelized_sphere(center: np.ndarray, radius: int):\n    \"\"\"Make sure radius reflects the size of the underlying voxel grid\"\"\"\n    x0, y0, z0 = center\n\n    #!------ Generate indices of a voxel cube of side 2r  around the centerpoint\n    x_range = slice(\n        int(np.floor(x0 - radius)), \n        int(np.ceil(x0 + radius)))\n    y_range = slice(\n        int(np.floor(y0 - radius)), \n        int(np.ceil(y0 + radius)))\n    z_range = slice(\n        int(np.floor(z0 - radius)), \n        int(np.ceil(z0 + radius)))\n\n    indices = np.indices(\n        (\n            x_range.stop - x_range.start,\n            y_range.stop - y_range.start,\n            z_range.stop - z_range.start,\n        )\n    )\n\n    indices      += np.array([x_range.start,\n                              y_range.start,\n                              z_range.start])[:, np.newaxis, np.newaxis, np.newaxis ]\n    indices       = indices.transpose(1, 2, 3, 0)\n    indices_list  = list(map(tuple, indices.reshape(-1, 3)))\n\n    #!------ Generate indices of a voxel cube of side 2r+2  around the centerpoint\n    sphere_active_ix = []\n\n    for ind in indices_list:\n        x_ = ind[0]\n        y_ = ind[1]\n        z_ = ind[2]\n        if (x_ - x0) ** 2 + (y_ - y0) ** 2 + (z_ - z0) ** 2 &lt;= radius**2:\n            sphere_active_ix.append([x_, y_, z_])\n\n    return np.array(sphere_active_ix)\n\n\n\n\n\n\n\n\n\n\nindex_grid: populate a voxel grid (with sphered atoms)\n\n\n\n\n\n\ndef index_grid(expanded_sphere_voxels: np.ndarray) :\n\n    def normalize_atom_coordinates(coordinates: np.ndarray)-&gt;tuple[ np.ndarray, np.ndarray ]:\n        \"\"\"@param coordinates: numpy array of shape (N,3)\"\"\"\n\n        C      = coordinates\n        mean_x = np.mean(C[:, 0])\n        mean_y = np.mean(C[:, 1])\n        mean_z = np.mean(C[:, 2])\n\n        Cx = C[:, 0] - mean_x\n        Cy = C[:, 1] - mean_y\n        Cz = C[:, 2] - mean_z\n        \n\n        [dev_x, dev_y, dev_z] = [np.min(Cx), np.min(Cy), np.min(Cz)]\n\n        #! shift to positive quadrant\n        Cx = Cx + abs(dev_x)\n        Cy = Cy + abs(dev_y)\n        Cz = Cz + abs(dev_z)\n\n        rescaled_coords = np.array(list(zip(Cx, Cy, Cz)))\n\n        return rescaled_coords, np.array([[mean_x,mean_y,mean_z], [abs( dev_x ), abs( dev_y ), abs( dev_z )]])\n\n    normalized_sphere_cords, mean_abs_vectors = normalize_atom_coordinates(expanded_sphere_voxels)\n    voxel_size = 1\n\n    sphere_cords_quantized = np.round(np.array(normalized_sphere_cords / voxel_size) ).astype(int)\n    max_values             = np.max(sphere_cords_quantized, axis=0)\n    grid_dimensions        = max_values + 1\n    vox_grid               = np.zeros(grid_dimensions)\n\n    print(\"Dimension of the voxel grid is \", vox_grid.shape)\n\n    vox_grid[\n        sphere_cords_quantized[:, 0],\n        sphere_cords_quantized[:, 1],\n        sphere_cords_quantized[:, 2]  ] = 1\n\n\n    return ( vox_grid, grid_dimensions, mean_abs_vectors )\n\n\n\n\nBbox: There are many ways to extract a point cloud from a larger biological structure – in this case we settle for a bounding box that bounds the space between the PTC and the NPET vestibule.\n\n# \"bounding_box_atoms.npy\" is a N,3 array of atom coordinates\n\natom_centers = np.load(\"bounding_box_atoms.npy\") \n\nSphering: To make the representation of atoms slightly more physically-plausible we replace each atom-center coordinate with positions of voxels that fall within a sphere of radius \\(R\\) around the atom’s position. This is meant to represent the atom’s van der Waals radius.\nOne could model different types of atoms (\\(N\\),\\(C\\),\\(O\\),\\(H\\) etc.) with separate radii, but taking \\(R=2\\) proves a good enough compromise. The units are Angstrom and correspond to the coordinate system in which the structure of the ribosome is recorded.\n\nvoxel_spheres = np.array([ atompos_to_voxel_sphere(atom, 2) for atom in atom_centers ])\n\nVoxelization & Inversion: Since we are interested in the “empty space” between the atoms, we need a way to capture it. To make this possible we discretize the space by projecting the (sphered) point cloud into a voxel grid and invert the grid.\n\n# the grid is a binary 3D-array \n# with 1s where a normalized 3D-coordinate of an atom corresponds to the cell index and 0s elsewhere\n\n# by \"normalized\" i mean that the atom coordinates are\n# temporarily moved to the origin to decrease the size of the grid (see `index_grid` method further).\ninitial_grid, grid_dims, _ = index_grid(voxel_spheres)\n\n# The grid is inverted by changing 0-&gt;1 and 1-&gt;0\n# Now the atom locations are the null voxels and the empty space is active voxels\ninverted_grid              = np.asarray(np.where(initial_grid != 1)).T\n\nCompare the following representation (Inverted Point Cloud) to the first point cloud: notice that where there previously was an active voxel is now an empty voxel and vice versa. The tubular constellation of active voxels in the center of the bounding box on this inverted grid is the tunnel “space” we are interested in.\n\n\n\n\n\n\n\n\n\n\n\n(a) Initial bounding-box point cloud\n\n\n\n\n\n\n\n\n\n\n\n(b) Inverted point cloud\n\n\n\n\n\n\n\nFigure 1: Pointcloud inversion via a voxel grid."
  },
  {
    "objectID": "posts/npet-poisson-recon/index.html#subcloud-extraction",
    "href": "posts/npet-poisson-recon/index.html#subcloud-extraction",
    "title": "3D tessellation of biomolecular cavities",
    "section": "2. Subcloud Extraction",
    "text": "2. Subcloud Extraction\n\n\n\n\n\n\nDBSCAN_capture\n\n\n\n\n\n\nfrom sklearn.cluster import DBSCAN\ndef DBSCAN_capture(\n    ptcloud: np.ndarray,\n    eps           ,\n    min_samples   ,\n    metric        : str = \"euclidean\",\n): \n\n    u_EPSILON     = eps\n    u_MIN_SAMPLES = min_samples\n    u_METRIC      = metric\n\n    print(\"Running DBSCAN on {} points. eps={}, min_samples={}, distance_metric={}\"\n    .format( len(ptcloud), u_EPSILON, u_MIN_SAMPLES, u_METRIC ) ) \n\n    db     = DBSCAN(eps=eps, min_samples=min_samples, metric=metric).fit(ptcloud) # &lt;-- this is all you need\n\n    labels = db.labels_\n\n    CLUSTERS_CONTAINER = {}\n    for point, label in zip(ptcloud, labels):\n        if label not in CLUSTERS_CONTAINER:\n            CLUSTERS_CONTAINER[label] = []\n        CLUSTERS_CONTAINER[label].append(point)\n\n    CLUSTERS_CONTAINER = dict(sorted(CLUSTERS_CONTAINER.items()))\n    return db, CLUSTERS_CONTAINER\n\n\n\n\n\n\n\n\n\n\nDBSCAN_pick_largest_cluster\n\n\n\n\n\n\nfrom sklearn.cluster import DBSCAN\ndef DBSCAN_pick_largest_cluster(clusters_container:dict[int,list])-&gt;np.ndarray:\n    DBSCAN_CLUSTER_ID = 0\n    for k, v in clusters_container.items():\n        if int(k) == -1:\n            continue\n        elif len(v) &gt; len(clusters_container[DBSCAN_CLUSTER_ID]):\n            DBSCAN_CLUSTER_ID = int(k)\n    return np.array(clusters_container[DBSCAN_CLUSTER_ID])\n\n\n\n\nClustering: Having obtained a voxelized representation of the interatomic spaces inside and around the NPET our task is now to extract only the space that corresponds to the NPET. We use DBSCAN.\nscikit’s implementation of DBSCAN conveniently lets us retrieve the points from the largest cluster only, which corresponds to the active voxels of NPET space (if we eyeballed our DBSCAN parameters well).\n\nfrom scikit.cluster import DBSCAN\n\n_u_EPSILON, _u_MIN_SAMPLES, _u_METRIC  = 5.5, 600, 'euclidian'\n\n_, clusters_container   = DBSCAN_capture(inverted_grid, _u_EPSILON, _u_MIN_SAMPLES, _u_METRIC ) \nlargest_cluster         = DBSCAN_pick_largest_cluster(clusters_container)\n\n\n\n\n\n\n\nDBSCAN Parameters and grid size.\n\n\n\n\n\nOur 1Å-side grid just happens to be granular enough to accomodate a “correct” separation of clusters for some empirically established values of min_nbrs and epsilon (DBSCAN parameters), where the largest cluster captures the tunnel space.\nA possible issue here is “extraneous” clusters merging into the cluster of interest and thereby corrupting its shape. In general this occurs when there are clusters of density that are close enough (within epsilon to the main one to warrant a merge) and simultaneously large enough that they fulfill the min_nbrs parameter. Hence it might be challenging to find the combination of min_nbrs and epsilon that is sensitive enough to capture the main cluster completely and yet discriminating enough to not subsume any adjacent clusters.\nIn theory, a finer voxel grid (finer – in relationship to the initial coordinates of the general point cloud; sub-angstrom in our case) would make finding the combination of parameters specific to the dataset easier: given that the atom-sphere would be represented by a proprotionally larger number of voxels, the euclidian distance calculation between two voxels would be less sensitive to the change in epsilon.\nPartioning the voxel grid further would come at a cost:\n\nyou would need to rewrite the sphering method for atoms (to account for the the new voxel-size)\nthe computational cost will increase dramatically, the dataset could conceivably stop fitting into memory alltogether.\n\n\n\n\n\n\n\nClusters identified by DBSCAN on the inverted index grid. The largest cluster corresponds to the tunnel space.\n\n\n\n\n\n\n\n\nSubcloud refinement\n\n\n\n\n\nI found that this first pass of DBSCAN (eps=\\(5.5\\), min_nbrs=\\(600\\)) successfully identifies the largest cluster with the tunnel but generally happens to be conservative in the amount of points that are merged into it. That is, there are still redundant points in this cluster that would make the eventual surface reconstruction spatially overlap with the rRNA and protiens. To “sharpen” this cluster we apply DBSCAN only to its sub-pointcloud and push the eps distance down to \\(3\\) and min_nbrs to \\(123\\) (again, “empirically established” values), which happens to be about the lowest parameter values at which any clusters form. This sharpened cluster is what the tesselation (surface reconstruction) will be performed on.\n\n\n\n\n\n\n\n\n\n\n\n(a) Largest DBSCAN cluster (trimmed from the vestibule side).\n\n\n\n\n\n\n\n\n\n\n\n(b) Cluster refinement: DBSCAN{e=3,mn=123} result (marine blue) on the largest cluster of DBSCAN{e=5.5,mn=600} (gray)\n\n\n\n\n\n\n\nFigure 2: Second pass of DBSCAN sharpens the cluster to peel off the outer layer of redundant points."
  },
  {
    "objectID": "posts/npet-poisson-recon/index.html#surface-reconstruction",
    "href": "posts/npet-poisson-recon/index.html#surface-reconstruction",
    "title": "3D tessellation of biomolecular cavities",
    "section": "3. Surface Reconstruction",
    "text": "3. Surface Reconstruction\n\n\n\n\n\n\nptcloud_convex_hull_points\n\n\n\n\n\nSurface points can be extracted by creating an alpha shape over the point cloud and taking only the points that belong to the alpha surface.\n\nimport pyvista as pv\nimport open3d as o3d\nimport numpy as np\n\ndef ptcloud_convex_hull_points(pointcloud: np.ndarray, ALPHA:float, TOLERANCE:float) -&gt; np.ndarray:\n    assert pointcloud is not None\n    cloud       = pv.PolyData(pointcloud)\n    grid        = cloud.delaunay_3d(alpha=ALPHA, tol=TOLERANCE, offset=2, progress_bar=True)\n    convex_hull = grid.extract_surface().cast_to_pointset()\n    return convex_hull.points\n\nOne could content themselves with the alpha shape representation of the NPET geometry and stop here, but it’s easy to notice that the vertice of the polygon (red dots) are distributed unevenly over the surface. This is likely to introduce artifacts and instabilities into further simulations.\n\n\n\n\n\n\n\n\n\n\n\n(a) Alpha-shape over the pointcloud\n\n\n\n\n\n\n\n\n\n\n\n(b) Surface points of the point cloud\n\n\n\n\n\n\n\nFigure 3: Alpha shape provides a way to identify surface points.\n\n\n\n\n\n\n\n\n\n\n\n\nestimate_normals\n\n\n\n\n\nNormal estimation is done via rolling a tangent plane over the surface points.\n\nimport pyvista as pv\nimport open3d as o3d\nimport numpy as np\n\ndef estimate_normals(convex_hull_surface_pts: np.ndarray, kdtree_radius=None, kdtree_max_nn=None, correction_tangent_planes_n=None): \n    pcd        = o3d.geometry.PointCloud()\n    pcd.points = o3d.utility.Vector3dVector(convex_hull_surface_pts)\n\n    pcd.estimate_normals(search_param=o3d.geometry.KDTreeSearchParamHybrid(radius=kdtree_radius, max_nn=kdtree_max_nn) )\n    pcd.orient_normals_consistent_tangent_plane(k=correction_tangent_planes_n)\n\n    return pcd\n\n\n\n\nNormals’ orientations are depicted as vectors(black) on each datapoint.\n\n\n\n\n\n\n\n\n\n\n\napply_poisson_recon\n\n\n\n\n\nThe source is available at https://github.com/mkazhdan/PoissonRecon. For programmability we connect the binary to the pipeline by wrapping it in a python subprocess but one can of course use the binary directly.\nThe output of the binary is a binary .ply (Stanford Triangle Format) file. For purposes of distribution we also produce an asciii-encoded version of this .ply file side-by-side: some geometry packages are only able to parse the ascii version.\n\ndef apply_poisson_reconstruction(surf_estimated_ptcloud_path: str, recon_depth:int=6, recon_pt_weight:int=3):\n    import plyfile\n    # The documentation can be found at https://www.cs.jhu.edu/~misha/Code/PoissonRecon/Version16.04/ in \"PoissonRecon\" binary\n    command = [\n        POISSON_RECON_BIN,\n        \"--in\",\n        surf_estimated_ptcloud_path,\n        \"--out\",\n        output_path,\n        \"--depth\",\n        str(recon_depth),\n        \"--pointWeight\",\n        str(recon_pt_weight),\n        \"--threads 8\"\n    ]\n    process = subprocess.run(command, capture_output=True, text=True)\n    if process.returncode == 0:\n        print(\"&gt;&gt;PoissonRecon executed successfully.\")\n        print(\"&gt;&gt;Wrote {}\".format(output_path))\n        # Convert the plyfile to asciii\n        data = plyfile.PlyData.read(output_path)\n        data.text = True\n        ascii_duplicate =output_path.split(\".\")[0] + \"_ascii.ply\"\n        data.write(ascii_duplicate)\n        print(\"&gt;&gt;Wrote {}\".format(ascii_duplicate))\n    else:\n        print(\"&gt;&gt;Error:\", process.stderr)\n\n\n\n\nThe final NPET surface reconstruction\n\n\n\n\n\nNow, having refined the largest DBSCAN cluster, we have a pointcloud which faithfully represent the tunnel geometry. To create a watertight mesh from this point cloud we need to prepare the dataset:\n\nretrieve only the “surface” points from the pointcloud\nestimate normals on the surface points (establish data orientation)\n\n\nd3d_alpha, d3d_tol     = 2, 1\n\nsurface_pts = ptcloud_convex_hull_points(coordinates_in_the_original_frame, d3d_alpha,d3d_tol)\npointcloud  = estimate_normals(surface_pts, kdtree_radius=10, kdtree_max_nn=15, correction_tangent_planes_n=10)\n\nThe dataset is now ready for surface reconstruction. We reach for Poisson surface reconstruction(Kazhdan, Bolitho, and Hoppe 2006) by Kazhdan and Hoppe, a de facto standard in the field.\n\nPR_depth , PR_ptweight = 6, 3\napply_poisson_recon(pointcloud, recon_depth=PR_depth, recon_pt_weight=PR_ptweight)\n\n\nPoisson Surface Reconstruction in a few Eqns:\nDefine basis functions space. \\(o.c\\), \\(o.w\\) are parameters of the octree: \\[\n\\begin{equation*}\n    F_o(q) \\equiv F\\left(\\frac{q-o.c}{o.w}\\right)\\frac{1}{o.w^3}\n\\end{equation*}\n\\]\nPick a basis function. \\(B\\) is just a “box-function”:\n\\[\n        \\begin{align*}\n            F(x,y,z) &\\equiv (B(x)B(y)B(z))^{*n} \\\\\n            \\text{with } B(t) &=\n            \\begin{cases}\n                1 & |t| &lt; 0.5 \\\\\n                0 & \\text{otherwise}\n            \\end{cases}\n        \\end{align*}\n\\]\nDefine vector field from points of your ptcloud: \\[\n        \\begin{equation*}\n            \\bar{V}(q) \\equiv \\sum_{s\\in S}\\sum_{o\\in\\text{Ngbr}_D(s)}\\alpha_{o,s}F_o(q)s.\\vec{N}\n        \\end{equation*}\n\\] \nSolve Poisson eqn. in \\(\\chi\\) and \\(V\\)(least squares): \\[\n        \\begin{align*}\n            \\sum_{o\\in\\mathcal{O}}\\|\\langle\\Delta\\tilde{\\chi}-\\nabla\\cdot\\bar{V},F_o\\rangle\\|^2 = \\\\\n            \\sum_{o\\in\\mathcal{O}}\\|\\langle\\Delta\\tilde{\\chi},F_o\\rangle-\\langle\\nabla\\cdot\\bar{V},F_o\\rangle\\|^2\n        \\end{align*}\n\\]\nTrace the isosurface through the solution (that’s your implicit surface): \\[\n        \\begin{align*}\n            \\partial\\tilde{M} &\\equiv \\{q\\in\\mathbb{R}^3 | \\tilde{\\chi}(q)=\\gamma\\} \\\\\n            \\text{with } \\gamma &= \\frac{1}{|S|}\\sum_{s\\in S}\\tilde{\\chi}(s.p)\n        \\end{align*}\n\\]"
  },
  {
    "objectID": "posts/npet-poisson-recon/index.html#result",
    "href": "posts/npet-poisson-recon/index.html#result",
    "title": "3D tessellation of biomolecular cavities",
    "section": "Result",
    "text": "Result\nWhat you are left with is a smooth polygonal mesh in the .ply format. Below is the illustration of the fidelity of the representation. Folds and depressions can clearly be seen engendered by three proteins surrounding parts of the tunnel (uL22 yellow, uL4 light blue and eL39 magenta). rRNA is not shown.Sullivan and Kaszynski (2019)\n\n\n\nThe NPET mesh surrounded by by three ribosome proteins"
  },
  {
    "objectID": "posts/npet-poisson-recon/index.html#improvements-needed-for-this-protocol",
    "href": "posts/npet-poisson-recon/index.html#improvements-needed-for-this-protocol",
    "title": "3D tessellation of biomolecular cavities",
    "section": "Improvements needed for this protocol:",
    "text": "Improvements needed for this protocol:\n\neliminate radial plot dependency. This could be done by fitting a plane to the tunnel vestibule through a few conserved sites and aligning it to be normal to the PTC, dropping a cylinder with a liberal radius between the plane the ptc.\nautomatic choice of cluster. for larger radii of bbox expansion the largest dbscan cluster doesnt necessarily correspond to the tunnel."
  },
  {
    "objectID": "WIP_posts/no_sidebar_molstar_attempt/index.html",
    "href": "WIP_posts/no_sidebar_molstar_attempt/index.html",
    "title": "Draft",
    "section": "",
    "text": "We present a protocol to extract the surface of a biomolecular cavity for shape analysis and molecular simulations.\nWe apply and illustrate the protocol on the ribosome structure, which contains a subcompartment known as the ribosome exit tunnel or “nascent polypeptide exit tunnel” (NPET). More details on the tunnel features and biological importance can be found in our previous works.\nThe protocol was designed to refine the output obtained from MOLE software, but can be applied to reconstruct a mesh on any general point cloud. Hence, we take the point-cloud of atom positions surrounding the tunnel as a point of departure."
  },
  {
    "objectID": "WIP_posts/no_sidebar_molstar_attempt/index.html#summary-and-background",
    "href": "WIP_posts/no_sidebar_molstar_attempt/index.html#summary-and-background",
    "title": "Draft",
    "section": "",
    "text": "We present a protocol to extract the surface of a biomolecular cavity for shape analysis and molecular simulations.\nWe apply and illustrate the protocol on the ribosome structure, which contains a subcompartment known as the ribosome exit tunnel or “nascent polypeptide exit tunnel” (NPET). More details on the tunnel features and biological importance can be found in our previous works.\nThe protocol was designed to refine the output obtained from MOLE software, but can be applied to reconstruct a mesh on any general point cloud. Hence, we take the point-cloud of atom positions surrounding the tunnel as a point of departure."
  },
  {
    "objectID": "WIP_posts/no_sidebar_molstar_attempt/index.html#pointcloud-preparation-bounding-box-and-voxelization",
    "href": "WIP_posts/no_sidebar_molstar_attempt/index.html#pointcloud-preparation-bounding-box-and-voxelization",
    "title": "Draft",
    "section": "1. Pointcloud Preparation: Bounding Box and Voxelization",
    "text": "1. Pointcloud Preparation: Bounding Box and Voxelization\nBbox: There are many ways to extract a point cloud from a larger biological structure – in this case we settle for a bounding box that bounds the space between the PTC and the NPET vestibule.\n\n  \n\n\n// Example JavaScript for loading a 3D visualization\n// This would be replaced with your actual visualization code\nfigure1_vis = {\n  const div = d3.select(\"#figure1\");\n  div.html(\"&lt;p&gt;Interactive 3D visualization would be inserted here&lt;/p&gt;\");\n  return div.node();\n}"
  },
  {
    "objectID": "WIP_posts/dotfiles/configs.html",
    "href": "WIP_posts/dotfiles/configs.html",
    "title": "Configs",
    "section": "",
    "text": "Configs\nkitty zsh lfcd (or ranger if go is unavailable) - preview rendering - ~/.config/lf_preview\nvimrc xbindkeys clipmenu vscode\nsioyek tmux"
  },
  {
    "objectID": "WIP_posts/tubulin/tubulin_notes/slides.html",
    "href": "WIP_posts/tubulin/tubulin_notes/slides.html",
    "title": "Intro Topics",
    "section": "",
    "text": "—- TO INVESTIGATE FURTHER —- #### OVERALL #### OVERALL\n\nIntro Topics\n\noutline the rough product that you are aiming for, an integrative platform that has a generally applicable library in the backend and a visualization platform.\nit’s important to index all avaialble data in a way that is easily accessible and searchable\nwould be nice for the method of indexing to be future-proof/extendable to new datasets.\nenumerate the datatypes/databases that you’ll be speaking about to draw some boundaries\n\n\n\nSequence Block\nIsotypes characterization by species & cell types PTMS characterization by species, cell types and isotypes\n\nStart with broad family HMMs (alpha, beta, etc.) For each family.\n\nCollect all matching sequences\nPerform clustering (try several thresholds)\nBuild phylogenetic trees to identify natural groups\nExtract C-terminal regions and cluster them separately\nCompare these different clustering results\n\n\nWhere clusters agree across methods, define these as strong sub-families Build HMMs for these sub-families Test with known examples to evaluate discrimination power Iteratively refine as needed\nThe above for families of tubulins and, separately, for families of MAPs. Stathmin-lke\n\n\nPTMs possibly included into clustering/families/searches via ProForma Notation\nsearch/discovery in a single modality instead of searching across various dbs, fasta files, mass spec records etc.\n\n\nHelices as domain “views” into families.\n\n\nControl Layer\n\nintroduce neo4j graph database as a way to keep track of semantic data and connect it to the structural models\n\n\n\nStructural Domain\nIndex PDB structures and incorporate them into the graph according to the sequence block.\n\nintroduce capabilities of molstar as the the visualizing applications.\nRaise interactivity-vs-generality tradeoff\nsearch/navigation/visualization/comparison of the above\ncomparison/alignment (across what? sequences)\nligand binding sites\n\nThen go more indepth to augmneting each of the individual following aspects of the data at scale:\n\nCreating 3D structures of proteins from from sequences via AlphaFold\n\n\nPTM Reconstruction workflows\nReconstructing PTMs on top of a template structure via:\n  - Rosetta/\n  - PSIptm\n\n\nLigand Binding Sites\nFragments \nLigands Classification via \nBinking Pokets, Sites\n\n\nApplications for Model building:\n- HMM families-based deep learning models for automating CryoEM model building :\n    https://www.nature.com/articles/s41586-024-07215-4\n    https://www.biorxiv.org/content/10.1101/2025.03.16.643561v1\n    https://www.biorxiv.org/content/10.1101/2024.11.13.623164v1.full.pdf\n\n\nMD applications"
  },
  {
    "objectID": "WIP_posts/tubulin/tubulin_notes/thoughts.html",
    "href": "WIP_posts/tubulin/tubulin_notes/thoughts.html",
    "title": "PTMS",
    "section": "",
    "text": "Tubulin Database Application Design From the perspective of the tubulin research community, here are the key elements your database application should incorporate: Structural Classifications\nIsotype tracking: Alpha/beta/gamma tubulin variants with their distinct structural features PTM mapping: Comprehensive post-translational modification sites (acetylation, tyrosination, polyglutamylation, etc.) Conformational states: Straight vs. curved, GTP vs. GDP-bound states Lattice arrangements: A vs. B lattice, protofilament number variations (13 vs. non-13) Interface classifications: Lateral and longitudinal contact regions Seam structures: Variations in microtubule seam architecture\nBiologically Relevant Operations\nPTM pattern search: Query by specific modification patterns Structural alignment tools: Compare conformational changes between states Pharmacological binding site analysis: Drug interaction surfaces MAP binding region identification: Sites for motor proteins and other MAPs Evolutionary conservation mapping: Highlight conserved regions across species Disease-associated mutation correlation: Link mutations to structural changes\nCritical Loci\nNucleotide binding pockets: GTP/GDP binding sites ## drug-binding domains ## Exchangeable vs. non-exchangeable nucleotide sites ## Highly variable regions subject to most PTMs ## Critical for polymerization dynamics Loop regions: T5, H1-S2, M, etc. that change conformation during dynamics\nAdditional Functionalities\nFiltering: Feature-navigable structural data Experimental method classification: Cryo-EM vs. X-ray crystallography sources Taxonomic organization: Species-specific tubulin structures Integrated visualization: Direct linking to molecular viewers with highlighting options\nWould you like me to expand on any particular aspect of this database design?\n\nPTMS\n\nAcetylation\nChemistry: Addition of an acetyl group (-COCH₃) Location: Primarily occurs on lysine-40 of α-tubulin in the microtubule lumen Persistence: Marks stable, long-lived microtubules Function: Increases microtubule mechanical resilience, affects kinesin binding Structures: PDB entries like 5JCO and 6U42 show acetylated tubulin\n\n\nDetyrosination/Tyrosination\nChemistry: Removal of C-terminal tyrosine from α-tubulin (detyrosination), followed by possible re-addition (tyrosination) Location: Extreme C-terminal tail of α-tubulin Persistence: Detyrosination accumulates on stable microtubules Function: Regulates interactions with +TIPs, CAP-Gly proteins, and some motors Structures: Mostly studied through biochemical methods as CTTs are often disordered in crystal structures\n\n\nGlutamylation\nChemistry: Addition of glutamate side chains (1-6+ units) Location: Glutamate residues in both α- and β-tubulin CTTs Persistence: Variable, can be dynamically regulated Function: Regulates motor trafficking, severing enzyme activity (spastin, katanin) Structures: Challenging to visualize in structural studies due to CTT flexibility\n\n\nGlycylation\nChemistry: Addition of glycine residues (1-34+ units) Location: Similar sites as glutamylation on CTTs Persistence: Predominant in stable ciliary and flagellar microtubules Function: Critical for ciliary and flagellar stability Structures: Limited structural data due to CTT flexibility\n\n\nPhosphorylation\nChemistry: Addition of phosphate group to serine, threonine, or tyrosine Location: Various sites including Ser172 on β-tubulin Persistence: Dynamic, often cell-cycle regulated Function: Can inhibit microtubule assembly, regulates dynamics Structures: Some phosphorylated sites visible in structures like 3EDL\n\n\nMethylation\nChemistry: Addition of methyl groups to lysine or arginine residues Location: Various sites in the globular domain Persistence: Less characterized than other PTMs Function: May affect protein-protein interactions Structures: Limited structural information\n\n\nPalmitoylation\nChemistry: Attachment of palmitic acid via thioester bond to cysteine Location: Cysteine residues, particularly Cys376 in α-tubulin Persistence: Reversible lipid modification Function: May affect membrane association Structures: Difficult to capture in crystal structures\n\n\nSuccinylation/Malonylation\nChemistry: Addition of succinyl or malonyl groups to lysine residues Location: Various lysine residues throughout tubulin Persistence: Newly characterized, less understood Function: May affect protein stability and interactions Structures: Limited structural data\n\n\nCitrullination\nChemistry: Conversion of arginine to citrulline Location: Arginine residues throughout tubulin Persistence: Generally stable modification Function: May affect microtubule stability Structures: Limited structural information\n\n\nMARylation (Mono-ADP-ribosylation)\nChemistry: Addition of ADP-ribose from NAD⁺ Location: Various residues including glutamate and arginine Persistence: Reversible, can be rapidly regulated Function: Often stress-responsive, can affect protein interactions Structures: Few structures available\nViewing These PTMs Most high-resolution structures of tubulin (like 1TUB, 6EVW, 5SYF) don’t include the complete CTTs where many PTMs occur, as these regions are intrinsically disordered. However, you can find structures capturing some PTMs:\nFor acetylated tubulin, check PDB entries like 5JCO For phosphorylated sites, structures like 3EDL show some sites The Protein Data Bank (PDB) contains numerous tubulin structures, but only a fraction capture PTMs\nFor many of these PTMs, structural visualization is challenging because:\nThe CTTs are highly flexible Many PTMs are transient Sample preparation for crystallography often removes these modifications\nResearchers often use molecular dynamics simulations to model how these PTMs might affect tubulin structure and interactions when direct structural data is unavailable.\n\n\n\nDatabases\nhttps://proteininformationresource.org/resid/ PhosphoSitePlus Tubulin Database\n#TODO: What formats do PTM databases and mass spec use?\n\n\n\nIsotypes\nYou’ve hit on an important point about the relationship between structural biology and the broader knowledge of tubulin biology. Your instinct is correct - this approach is actually becoming more realistic and is happening in advanced tubulin research.\nWhat you’re describing is essentially a form of “computational reconstruction” or “biologically-informed modeling” that combines:\nThe structural scaffold from cryo-EM\nBiochemical/proteomic knowledge of isotype composition\nPrior knowledge about isotype-specific sequence differences\nThis approach is indeed being used by specialized tubulin researchers. Here’s how it works in practice:\nMass spectrometry or other techniques determine the precise isotype composition of a particular cellular structure (e.g., \"this cilium contains 60% TUBA1A, 25% TUBA4A, 10% TUBA8, 5% TUBA3C\")\nThe cryo-EM structure provides the overall architecture and placement of subunits\nResearchers can then create computational models that distribute these isotypes according to:\n    Known preferences for certain positions in the lattice\n    Known partner preferences (some α-isotypes preferentially pair with specific β-isotypes)\n    Experimentally determined ratios\nThis isn’t routine yet, but specialized labs focusing on tubulin biology do perform this kind of reconstruction for specific questions. It’s particularly relevant when studying:\nHow drugs interact differently with specific isotypes\nHow certain mutations in specific isotypes cause disease\nHow post-translational modifications interact with isotype-specific sequences\nThe challenge remains that we can’t definitively say “this specific subunit at position X in this specific microtubule is isotype Y” from the cryo-EM data alone. But the kind of informed reconstruction you’re describing represents the cutting edge of integrating structural and biochemical data in tubulin research.\n\nHow different isotypes are resolved structurally\n\nTypically, the starting scaffold is one of:\nConsensus sequence: A synthetic sequence representing the most common residue at each position across all relevant isotypes.\nDominant isotype: The most abundant isotype in the sample (e.g., TUBA1A for brain microtubules) serves as the template.\nReference structure: Sometimes a well-resolved X-ray structure (often from bovine or porcine tubulin) is used, even if it’s not identical to the isotypes being studied.\nReconstruction/Modeling Approaches\n\nThe transformation from scaffold to isotype-specific models typically involves:\nHomology modeling: Using tools like MODELLER, Rosetta, or AlphaFold to generate structural predictions for specific isotypes based on the template.\nFlexible fitting: The homology models are then fitted into the cryo-EM density with tools like MDFF (Molecular Dynamics Flexible Fitting), Flex-EM, or Phenix real-space refinement.\nEnergy minimization: MD simulations help resolve steric clashes and optimize the structure.\nThe key difference from standard homology modeling is that the cryo-EM density serves as an additional constraint - the model must both satisfy physics-based energy terms and fit the experimental density. Handling Structural Differences\nYou’re absolutely right that sequence differences can engender structural changes. This is handled in a few ways:\nLocal refinement: Areas with sequence differences get more extensive sampling and refinement.\nEnsemble modeling: Sometimes multiple conformations are modeled for regions with significant differences.\nValidation: Using MD simulations to test if the modified structure remains stable.\nA/B interfaces ?\nso, it appears that ptms are somewhat inaccisible within a lot of cryoem strcutrues due to resolution. at least for now and at least not fully and cosistently (though some papers do resolve PTMs like acetylated K40: https://www.cell.com/action/showPdf?pii=S0092-8674%2819%2931081-5). How can we “repair” these structures to be able to display and have faithful structural references(at least up to the veracity of the upstream data ex. seq and PTM). Can we “inject” PTMs dynamically into a template structure and then post-repair the strucutre locally with, say, rosetta?\n\n\n\nPTMs\n\nStructure tools that could enable ptms recovery/exploration in atomic structures.\nSort vis-a-vis the applications (downstream MD? MAP binding? Dynamics? how is this all done?)\n\n\nwhat volumes and diversities are avaiable routinely via Mass Spec and other methods in terms of seqeunces and resolution"
  },
  {
    "objectID": "WIP_posts/mmcif_stuff/czii_zarr_and_annotations.html",
    "href": "WIP_posts/mmcif_stuff/czii_zarr_and_annotations.html",
    "title": "артём",
    "section": "",
    "text": "https://github.com/copick/copick https://link.springer.com/article/10.1007/s00418-023-02209-1 https://link.springer.com/article/10.1007/s00418-023-02209-1 The CZ CryoET Data Portal: Enabling Collaborative Analysis of Standardized CryoET Data using Machine Learning Methods https://academic.oup.com/mam/article/31/Supplement_1/ozaf048.471/8212545 A realistic phantom dataset for benchmarking cryo-ET data annotation https://www.nature.com/articles/s41592-025-02800-5"
  },
  {
    "objectID": "WIP_posts/mmcif_stuff/mmcif.html",
    "href": "WIP_posts/mmcif_stuff/mmcif.html",
    "title": "Background | The Unfurling Landscape of Structural Biology",
    "section": "",
    "text": "Rewriting the file “on the fly” is hard due to lack of modularity and other:"
  },
  {
    "objectID": "WIP_posts/mmcif_stuff/mmcif.html#format-specs",
    "href": "WIP_posts/mmcif_stuff/mmcif.html#format-specs",
    "title": "Background | The Unfurling Landscape of Structural Biology",
    "section": "Format Specs",
    "text": "Format Specs\nhttps://www.iucr.org/resources/cif https://datascience.codata.org/articles/10.5334/dsj-2016-003"
  },
  {
    "objectID": "WIP_posts/mmcif_stuff/mmcif.html#formal-specs",
    "href": "WIP_posts/mmcif_stuff/mmcif.html#formal-specs",
    "title": "Background | The Unfurling Landscape of Structural Biology",
    "section": "Formal specs",
    "text": "Formal specs\nhttps://www.iucr.org/resources/cif/cif2 https://www.iucr.org/resources/cif/spec/version1.1 https://legacy.ccp4.ac.uk/newsletters/newsletter37/13_harvest.html"
  },
  {
    "objectID": "WIP_posts/mmcif_stuff/mmcif.html#implementations-and-random-docs",
    "href": "WIP_posts/mmcif_stuff/mmcif.html#implementations-and-random-docs",
    "title": "Background | The Unfurling Landscape of Structural Biology",
    "section": "Implementations and random docs",
    "text": "Implementations and random docs\nhttps://www.ccp4.ac.uk/html/cciflib.html http://comcifs.github.io/cif_api/index.html"
  },
  {
    "objectID": "WIP_posts/mmcif_stuff/mmcif.html#history-of",
    "href": "WIP_posts/mmcif_stuff/mmcif.html#history-of",
    "title": "Background | The Unfurling Landscape of Structural Biology",
    "section": "History of",
    "text": "History of\nhttps://www.ccp4.ac.uk/html/mmcifformat.html\n\nImplementation of Data Harvesting in the CCP4 Suite : https://legacy.ccp4.ac.uk/newsletters/newsletter37/13_harvest.html\n\n“HARVESTING” :https://www.ccp4.ac.uk/html/harvesting.html\n\nComplaining\nformat spec is ugly, says biopython developer: https://stackoverflow.com/a/11686524/10697358 People routinely write their own parsers to reconcile these and curse the design choices and lack of documentation. Ex. here are guys from model-angelo ( GNN for model building that chenwei looked at) fixing this on their onw : https://github.com/3dem/model-angelo/issues/51 Here is Deep Mind implementing their own parser https://huggingface.co/spaces/simonduerr/ProteinMPNN/blob/f969e9cfb6f11ba299c7108aadff124e9cf38b1f/alphafold/alphafold/data/mmcif_parsing.py Here the gemmi guy trying to connect his work to chimerax and stumbling over this : https://mail.cgl.ucsf.edu/mailman/archives/list/chimerax-users@cgl.ucsf.edu/thread/XOO3G5MUOHLQBHSVF2NENWYNG3SOUF2O/ https://bioinformatics.stackexchange.com/questions/14210/pdb-residue-numbering\nOld tune, but the eminence of AF/Rosetta/etc. recognized with the nobel (not saying it’s right or wrong) contrasted to the quality of the “dataset” they spring from, which is just what the PDB is increasingly is pretty staggering to me. They are stuck in 2000s. I’d bet an eye that if not for the institutional inertia and the OneDeposition workflow, a single company like Meta or HuggingFace or one of amazon’s 100 bio divisions could come up with a better solution in a span of year. By better i mean support for binary, direct integration with source em-maps, sane modern format, biological hierarchy integration and much MUCH richer data points: in terms of landmarks and shapes like we are doing or something more chemically minded a la Wilson/Polikanov or something dynamism-minded a la cryodrgn. It is crazy crazy crazy that people use 5-10mil$ machines from the cutting edge of science, thousands of dollars worth of cloud computer and i dont know how many biologist human hours to arrive at a crappy plaintext file almost entirely untethered to the fabric from which it came.\nI think every day about what CZII is going to do the moment they are consistely getting 3-4A subtomograms and need to “deposit” 1000 ribosomes at once for some horizontal study of a mouse brain cancer cell or something. Whatever it is it’s not PDB.\n\njust shitty documentation, no automatic schema, ad-hoc updates, no explanation for datatypes\n\nhttps://github.com/google-deepmind/alphafold/issues/252\nhttps://github.com/wwpdb-dictionaries/mmcif_pdbx/issues\nEx. here are guys from model-angelo ( GNN for model building that chenwei looked at) fixing this on their onw : https://github.com/3dem/model-angelo/issues/51\nHere is Deep Mind implementing their own parser https://huggingface.co/spaces/simonduerr/ProteinMPNN/blob/f969e9cfb6f11ba299c7108aadff124e9cf38b1f/alphafold/alphafold/data/mmcif_parsing.py\nHere the gemmi guy trying to connect his work to chimerax and stumbling over this : https://mail.cgl.ucsf.edu/mailman/archives/list/chimerax-users@cgl.ucsf.edu/thread/XOO3G5MUOHLQBHSVF2NENWYNG3SOUF2O/\nhttps://bioinformatics.stackexchange.com/questions/14210/pdb-residue-numbering\nhttps://proteopedia.org/wiki/index.php/Unusual_sequence_numbering ““”\n\n5afi.y is a trna sequence but is tagged with “F” at the end just because it carreis a phenylalanine. How crazy is that. No respect for the standardization of RNA alphabet.\n\nhttps://docs.rosettacommons.org/docs/latest/development_documentation/tutorials/robust\nStephanie Wankowitz:\nhttps://diffuse.science/posts/encoding/ –&gt; https://pmc.ncbi.nlm.nih.gov/articles/PMC11220883/pdf/m-11-00494.pdf https://x.com/stephanie_mul/status/1955319804539150665\nTo go over: -https://pubmed.ncbi.nlm.nih.gov/40586518/ -https://pmc.ncbi.nlm.nih.gov/articles/PMC12208665/pdf/elife-103797.pdf -https://arxiv.org/pdf/2505.01919\nTranslation, Librations, Screws https://onlinelibrary.wiley.com/doi/abs/10.1107/S0567740868001718\nDiego del Alamo:\nTo give one example, if you receive a model of an antigen against which you want to design antibodies, and some of the surface loops look weird, you'd like to know if they are justified by experimental data or were modeled w/ Rosetta, AF3, etc, ...\n\n.. because you don't want to design against some computational artifact. If you got the model from a human, that's fine, you can go ask. But structbio agents are ephemeral and can't be consulted a week later. So ideally that info gets stuffed in the PDB/CIF header. ...\n\n... But then that raises the question of how much metadata you can pack in there. Are you putting in MSAs? Etc. Anyway my guess is that this will soon be scaled up such that human supervision becomes unwieldy, so an agent should be able to answer these kinds of questions\nGabriele Corso:\n“Is anyone developing open-source MCP frameworks for llms to parse and understand biomolecular structures accurately? If so, please reach out to me, i would love to discuss, help and support!”\n\nML features:\n\nNative graph representations with k-nearest neighbor graphs, edge type annotations, and node features for GNNs like GearNet and DeepRank-GNN\nMulti-resolution tokenization (AlphaFold 3 uses flexible schemes: standard residues as single tokens, modified residues as atoms, ligands as individual atoms)\nPre-computed MSA embeddings as feature channels\nConfidence metrics (pLDDT, PAE) as first-class data alongside coordinates\nVersioned model parameters linking predictions to the networks that generated them\n\n? Can this also be used a “paging cache” for the QKV store of transformers etc, msa embeddings that would facilitate modern models’ operation?"
  },
  {
    "objectID": "listings/other_blogs.html",
    "href": "listings/other_blogs.html",
    "title": "Other blogs:",
    "section": "",
    "text": "https://en.wikipedia.org/wiki/The_Two_Cultures Feyrabend"
  },
  {
    "objectID": "listings/other_blogs.html#stuff-i-havent-read-but-need-to-remeber",
    "href": "listings/other_blogs.html#stuff-i-havent-read-but-need-to-remeber",
    "title": "Other blogs:",
    "section": "",
    "text": "https://en.wikipedia.org/wiki/The_Two_Cultures Feyrabend"
  },
  {
    "objectID": "listings/some_pieces.html",
    "href": "listings/some_pieces.html",
    "title": "артём",
    "section": "",
    "text": "Jim Hall sextet: Concierto De Aranjuez\nGil Evans: Las Vegas Tango\nArvo Part: Stabat Mater (Le Nuove Musiche, Krijn Koetsveld, Wendy Roobol)\nKara-Lis Coverdale: Grafts\nFloating Points: Promises\nBrian Eno: The Ship\nLino Capra Vaccina: Andante Ancestrale\nDoubling Riders: I Graffiti di Orione e Delle Pleiade (Dedicated to Henri Lhote)\nArvo Part: Fratres (Gidon Kremer & Keith Jarrett)\nPharoah Sanders: Harvest Time\nTraditional: Black is The Color of My True Love’s Hair (Marc Johnson, Bill Frisell, John Scofield, Peter Erskine)\nRupert Clervaux, Sian Ahern: In Shadowlands of Like and Lineness\nJordan De La Sierra: Temple of Aesthetic Action\nLaraaji: Unicorns in Paradise\nJohn Luther Adams: Become Ocean"
  },
  {
    "objectID": "listings.html#music-vids",
    "href": "listings.html#music-vids",
    "title": "артём",
    "section": "music vids",
    "text": "music vids\n\nLA NEGRA EN UTRERA. Al toque: Camarón, Moraito y Raimundo Amador\nhttps://www.youtube.com/watch?v=Ir9qgbpJSMk&list=RD4SksJKyk6bY\nhttps://www.youtube.com/watch?v=TMbPMmOI9Lo&list=RDTMbPMmOI9Lo\nhttps://www.youtube.com/watch?v=lbKsf8vQLVk&list=RDlbKsf8vQLVk\nhttps://www.youtube.com/watch?v=WHI2nWdRdXw&list=RDWHI2nWdRdXw\nhttps://www.youtube.com/watch?v=JnpyCEUESEw&list=RDJnpyCEUESEw\nhttps://www.youtube.com/watch?v=u0DOSMrvQB4&list=RDu0DOSMrvQB4\nhttps://www.youtube.com/watch?v=D0RPSDEOxTA"
  },
  {
    "objectID": "listings.html#top-x",
    "href": "listings.html#top-x",
    "title": "артём",
    "section": "top x",
    "text": "top x\n\nJim Hall sextet: Concierto De Aranjuez\nGil Evans: Las Vegas Tango\nArvo Part: Stabat Mater (Le Nuove Musiche, Krijn Koetsveld, Wendy Roobol)\nKara-Lis Coverdale: Grafts\nFloating Points: Promises\nBrian Eno: The Ship\nLino Capra Vaccina: Andante Ancestrale\nDoubling Riders: I Graffiti di Orione e Delle Pleiade (Dedicated to Henri Lhote)\nArvo Part: Fratres (Gidon Kremer & Keith Jarrett)\nPharoah Sanders: Harvest Time\nTraditional: Black is The Color of My True Love’s Hair (Marc Johnson, Bill Frisell, John Scofield, Peter Erskine)\nRupert Clervaux, Sian Ahern: In Shadowlands of Like and Lineness\nJordan De La Sierra: Temple of Aesthetic Action\nLaraaji: Unicorns in Paradise\nJohn Luther Adams: Become Ocean\nTim Hecker: Radio Amor"
  },
  {
    "objectID": "listings.html#blogs",
    "href": "listings.html#blogs",
    "title": "артём",
    "section": "blogs",
    "text": "blogs\n\nhttps://hyperparameter.space\nhttps://www.lesswrong.com/bestoflesswrong\nhttps://www.privatdozent.co/\nhttps://www.astralcodexten.com/\nhttps://distill.pub\nhttps://colah.github.io/posts\nhttps://karpathy.github.io\nhttps://ghost.oxen.ai\nhttps://www.aibiodesign.com/p\nhttps://atelfo.github.io/\nhttps://markovbio.github.io/\nhttp://gusevlab.org/projects/hsq/#h.7g8y898qpkvy\nhttps://www.jeremykun.com/2015/04/06/markov-chain-monte-carlo-without-all-the-bullshit/\nhttps://iwatobipen.wordpress.com/\nhttps://alchemybio.substack.com/p\nhttps://gwern.net/backstop"
  },
  {
    "objectID": "listings.html#landmarks",
    "href": "listings.html#landmarks",
    "title": "артём",
    "section": "landmarks",
    "text": "landmarks\n\nCan one hear the shape of a drum\nPolitics and the English Language, Orwell\nthe hedgehog and the fox"
  },
  {
    "objectID": "listings.html#stishki",
    "href": "listings.html#stishki",
    "title": "артём",
    "section": "stishki",
    "text": "stishki\n\n\nжизнь жизнь\nNabokov - Pale Fire\nduino elegies\nшесть лет спустя\non raglan road\nnerva l - the poem from foucault\nВы хожу один я на дорогу\nletters from iceland, auden\nwhatt, beckett\noh lovely appearance of death\nbabochka\nstrand, the couple\nБожественная Комедия в переводе Лозинского"
  },
  {
    "objectID": "listings.html#stuff",
    "href": "listings.html#stuff",
    "title": "артём",
    "section": "stuff",
    "text": "stuff\n\nGaucho doc\nshackleton, endurance\nwhen turing and wittgenstein met\nmetternich letters to dorothea von lieven\nrecoltes et semailles"
  }
]